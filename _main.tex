% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{book}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\ifLuaTeX
  \usepackage{luacolor}
  \usepackage[soul]{lua-ul}
\else
  \usepackage{soul}
\fi
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{booktabs}

\usepackage{color}
\usepackage{framed}
\setlength{\fboxsep}{.8em}

% These colours were manually entered, they shouldn't matter unless you want pdf output

\newenvironment{redbox}{
  \definecolor{shadecolor}{RGB}{243, 154, 157}
  \color{white}
  \begin{shaded}}
 {\end{shaded}}

\newenvironment{bluebox}{
  \definecolor{shadecolor}{RGB}{172, 210, 237}
  \color{white}
  \begin{shaded}}
 {\end{shaded}}

\newenvironment{greenbox}{
  \definecolor{shadecolor}{RGB}{141, 181, 128}
  \color{white}
  \begin{shaded}}
 {\end{shaded}}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={IDE Module 5 Lab},
  pdfauthor={Angela McLaughlin and Finlay Maguire},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{IDE Module 5 Lab}
\author{Angela McLaughlin and Finlay Maguire}
\date{November 24-26, 2025}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\part{Introduction}\label{part-introduction}

\chapter{Workshop Info}\label{workshop-info}

Welcome to the 2025 Pathogen Genomic Epidemiology Canadian Bioinformatics Workshop webpage!

\section{Pre-work}\label{pre-work}

\section{Class Photo}\label{class-photo}

Coming soon!

\section{Schedule}\label{schedule}

\chapter{Meet Your Faculty}\label{meet-your-faculty}

\subsubsection{Angela McLaughlin}\label{angela-mclaughlin}

\begin{quote}
Postdoctoral Fellow
Dalhousie University and University of Guelph
Burnaby, BC, Canada

--- \href{mailto:ez928230@dal.ca}{\nolinkurl{ez928230@dal.ca}}
\end{quote}

Angela McLaughlin is a postdoctoral fellow in Dr.~Finlay Maguire's lab at Dalhousie University, in collaboration with Dr.~Zvonimir Poljak at University of Guelph. Her research interests span viral phylogenetics (HIV-1, SARS-CoV-2, and influenza virus), genomic epidemiology, bioinformatics, public health, wildlife surveillance, and statistical/machine learning/mathematical models of pathogen transmission. Her current project aims to predict host specificity of avian influenza virus H5Nx using machine learning models of viral genomic features with phylogenetics-informed cross-validation and hierarchical segment to whole genome ensemble models.

\subsubsection{Emma Griffiths}\label{emma-griffiths}

\begin{quote}
Research Associate, Faculty of Health Sciences
Simon Fraser University
Vancouver, BC, Canada

--- \href{mailto:emma_griffiths@sfu.ca}{\nolinkurl{emma\_griffiths@sfu.ca}}
\end{quote}

Emma Griffiths is a research associate at the Centre for Infectious Disease Genomics and One Health (CIDGOH) in the Faculty of Health Sciences at Simon Fraser University in Vancouver, Canada. Her work focuses on developing and implementing ontologies and data standards for public health and food safety genomics to help improve data harmonization and integration. She is a member of the Standards Council of Canada and leads the Public Health Alliance for Genomic Epidemiology (PHA4GE) Data Structures Working Group.

\subsubsection{Finlay Maguire}\label{finlay-maguire}

\begin{quote}
Assistant Professor, Faculty of Computer Science and Department of Community Health \& Epidemiology,
Dalhousie University
Halifax, NS, Canada

--- \href{mailto:finlay.maguire@dal.ca}{\nolinkurl{finlay.maguire@dal.ca}}, finlaymagui.re
\end{quote}

Finlay Maguire is a genomic epidemiologist whose work centers on leveraging data in innovative ways to answer questions related to applied health and social issues. This includes developing bioinformatics methods to more effectively use genomic data to mitigate infectious diseases and broad interdisciplinary collaborations in areas such as refugee healthcare provision and online radicalisation. They are an active contributor to the national and international public health responses to emerging viral zoonoses and antimicrobial resistance, co-chair of the PHA4GE data structures working group, and act as a Pathogenomics Bioinformatics Lead for Sunnybrook's Shared Hospital Laboratory.

\subsubsection{Gary Van Domselaar}\label{gary-van-domselaar}

\begin{quote}
Chief, Bioinformatics, National Microbiology Laboratory
Public Health Agency of Canada
Winnipeg, MB, Canada

--- \href{mailto:gary.vandomselaar@phac-aspc.gc.ca}{\nolinkurl{gary.vandomselaar@phac-aspc.gc.ca}}
\end{quote}

Dr.~Gary Van Domselaar, PhD (University of Alberta, 2003) is the Chief of the Bioinformatics Section at the National Microbiology Laboratory in Winnipeg Canada and Associate Professor in the Department of Medical Microbiology and Infectious Diseases at the University of Manitoba. Dr.~Van Domselaar's lab develops bioinformatics methods and pipelines to understand, track, and control circulating infectious diseases in Canada and globally. His research and development activities span metagenomics, infectious disease genomic epidemiology, genome annotation, population structure analysis, and microbial genome wide association studies.

\subsubsection{Idowu Olawoye}\label{idowu-olawoye}

\begin{quote}
Postdoctoral Associate
University of Western Ontario
London, ON, Canada

--- \href{mailto:iolawoye@uwo.ca}{\nolinkurl{iolawoye@uwo.ca}}
\end{quote}

Idowu is a postdoctoral associate in the Guthrie Lab at the University of Western Ontario. His research focuses on utilizing computational biology to understand transmission patterns, genomic evolution, and antimicrobial resistance of bacterial pathogens. He has been involved in numerous bioinformatics workshops and trainings across Africa and most recently in Canada.

\subsubsection{Jennifer Guthrie}\label{jennifer-guthrie}

\begin{quote}
Assistant Professor
University of Western Ontario
London, ON, Canada

--- \href{mailto:jennifer.guthrie@uwo.ca}{\nolinkurl{jennifer.guthrie@uwo.ca}}
\end{quote}

Dr.~Jennifer Guthrie is a Canada Research Chair in Pathogen Genomics and Bioinformatics and Assistant Professor in the Departments of Microbiology \& Immunology and Epidemiology \& Biostatistics at Western University; she also serves as an Adjunct Scientist at Public Health Ontario. A genomic epidemiologist by training, in her research Dr.~Guthrie uses interdisciplinary approaches combining principles from data science and bioinformatics, and more traditional epidemiology and microbiology methods to further our understanding of disease transmission dynamics, antimicrobial resistance, and epidemiological characteristics of pathogens. Her research involves pathogens of public health importance such as SARS-CoV-2, influenza, Mycobacterium tuberculosis, and methicillin resistant Staphylococcus aureus.

\subsubsection{Zhibin Lu}\label{zhibin-lu}

\begin{quote}
Senior Manager, Digital Research University Health Network
Toronto, ON, Canada ---
\href{mailto:zhibin@gmail.com}{\nolinkurl{zhibin@gmail.com}}
\end{quote}

Zhibin Lu is a senior manager at University Health Network Digital. He is responsible for UHN HPC operations and scientific software. He manages two HPC clusters at UHN, including system administration, user management, and maintenance of bioinformatics tools for HPC4health. He is also skilled in Next-Gen sequence data analysis and has developed and maintained bioinformatics pipelines at the Bioinformatics and HPC Core. He is a member of the Digital Research Alliance of Canada Bioinformatics National Team and Scheduling National Team.

\subsubsection{Charlie Barclay}\label{charlie-barclay}

\begin{quote}
MSc Graduate Student Researcher
Simon Fraser University Vancouver, BC, Canada ---
\href{mailto:charlie_barclay@sfu.ca}{\nolinkurl{charlie\_barclay@sfu.ca}}
\end{quote}

Charlie Barclay is an ontologist at the Centre for Infectious Disease Genomics and One Health (CIDGOH), with expertise in data structures and modelling across pathogen genomics, public health, and biodiversity. She specialises in ontologies and metadata frameworks to standardise and integrate data in support of FAIR principles. Passionate about the intersection of environmental pressures and infectious disease, and advocates for a holistic One Health approach, integrating environmental monitoring with genomic data.

\chapter{Data and Compute Setup}\label{data-and-compute-setup}

\subsubsection{Course data downloads}\label{course-data-downloads}

Coming soon!

\subsubsection{Compute setup}\label{compute-setup}

Coming soon!

\chapter{Module 1 Data Curation and Data Sharing}\label{module-1-data-curation-and-data-sharing}

\section{Lecture}\label{lecture}

\section{Lab}\label{lab}

\subsection{Data Download}\label{data-download}

\href{https://drive.google.com/file/d/1mfLHfzPVOA6avBEoFhiAsIMfXuBrIMQp/view}{dataharmonizer}

\chapter{Module 2 Emerging Pathogen Detection and Identification}\label{module-2-emerging-pathogen-detection-and-identification}

\section{Lecture}\label{lecture-1}

\section{Lab}\label{lab-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \hyperref[intro]{Introduction}
\item
  \hyperref[software]{Software}\\
\item
  \hyperref[setup]{Setup}
\item
  \hyperref[exercise]{Exercise}

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \hyperref[exercise-background]{Patient Background}
  \item
    \hyperref[exercise-overview]{Overview}
  \item
    \hyperref[assembly-free]{Assembly-free approach}

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      \hyperref[exercise-examine-reads]{Step 1: Examine the reads}
    \item
      \hyperref[exercise-quality-reads]{Step 2: Clean and examine quality of the reads}
    \item
      \hyperref[exercise-host-filtering]{Step 3: Host read filtering}
    \item
      \hyperref[exercise-classify-kraken]{Step 4: Classify reads using Kraken2 database}
    \item
      \hyperref[exercise-pavian]{Step 5: Generate an interactive html-based report using Pavian}
    \end{enumerate}
  \item
    \hyperref[assembly-based]{Assembly-based approach}

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      \hyperref[exercise-metatranscriptomic-assembly]{Step 6: Metatranscriptomic assembly}
    \item
      \hyperref[exercise-evaluate-assembly]{Step 7: Evaluate assembly with Quast}
    \item
      \hyperref[exercise-blast]{Step 8: Using BLAST to look for existing organisms}
    \end{enumerate}
  \end{enumerate}
\item
  \hyperref[final]{Final words}
\end{enumerate}

\subsection{1. Introduction}\label{intro}

This tutorial aims to introduce a variety of software and concepts related to detecting emerging pathogens from a complex host sample. The provided data and methods are derived from real-world data, but have been modified to either illustrate a specific learning objective or to reduce the complexity of the problem. Contamination and a lack of large and accurate databases render detection of microbial pathogens difficult. As a disclaimer, all results produced from the tools described in this tutorial and others must also be verified with supplementary bioinformatics or wet-laboratory techniques.

\subsection{2. List of software for tutorial and its respective documentation}\label{software}

\begin{itemize}
\tightlist
\item
  \href{https://github.com/OpenGene/fastp}{fastp}
\item
  \href{https://multiqc.info/}{multiqc}
\item
  \href{https://kat.readthedocs.io/en/latest/}{KAT}
\item
  \href{https://ccb.jhu.edu/software/kraken2/}{Kraken2}
\item
  \href{https://fbreitwieser.shinyapps.io/pavian/}{Pavian}
\item
  \href{https://github.com/voutcn/megahit}{MEGAHIT}
\item
  \href{http://quast.sourceforge.net/quast}{Quast}
\item
  \href{https://blast.ncbi.nlm.nih.gov/Blast.cgi}{NCBI blast}
\end{itemize}

The workshop machines already have this software installed within a conda environment, but to perform this analysis later on, you can make use of the conda environment located at \url{environment.yml} to install the necessary software.

\subsection{3. Exercise setup}\label{setup}

\subsubsection{3.1. Copy data files}\label{copy-data-files}

To begin, we will copy over the exercises to \texttt{\textasciitilde{}/workspace}. This let's use view the resulting output files in a web browser.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cp} \AttributeTok{{-}r}\NormalTok{ \textasciitilde{}/CourseData/module2/module2\_workspace/ \textasciitilde{}/workspace/}
\BuiltInTok{cd}\NormalTok{ \textasciitilde{}/workspace/module2\_workspace/analysis}
\end{Highlighting}
\end{Shaded}

When you are finished with these steps you should be inside the directory \texttt{/home/ubuntu/workspace/module2\_workspace/analysis}. You can verify this by running the command \texttt{pwd}.

\textbf{Output after running \texttt{pwd}}

\begin{verbatim}
/home/ubuntu/workspace/module2_workspace/analysis
\end{verbatim}

You should also have a directory like \texttt{data/} one directory up from here. To check this, you can run \texttt{ls\ ../}:

\textbf{Output after running \texttt{ls\ ../}}

\begin{verbatim}
analysis  data  precomputed-analysis
\end{verbatim}

\subsubsection{3.2. Activate environment}\label{activate-environment}

Next we will activate the \href{https://docs.conda.io/en/latest/}{conda} environment, which will have all the tools needed by this tutorial pre-installed. To do this please run the following:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate module2{-}emerging{-}pathogen}
\end{Highlighting}
\end{Shaded}

You should see the command-prompt (where you type commands) switch to include \texttt{(module2-emerging-pathogen)} at the beginning, showing you are inside this environment. You should also be able to run one of the commands like \texttt{kraken2\ -\/-version} and see output:

\textbf{Output after running \texttt{kraken2\ -\/-version}}

\begin{verbatim}
Kraken version 2.17.1
Copyright 2013-2023, Derrick Wood (dwood@cs.jhu.edu)
\end{verbatim}

\subsubsection{3.3. Verify your workshop machine URL}\label{verify-your-workshop-machine-url}

This exercise will produce output files intended to be viewed in a web browser. These should be accessible by going to \url{http://xx.uhn-hpc.ca} in your web browser where \textbf{xx} is your particular number (like 01, 02, etc). If you are able to view a list of files and directories, try clicking the link for \textbf{module2\_workspace}. This page will be referred to later to view some of our output files. In addition, the link \textbf{precompuated-analysis} will contain all the files we will generate during this lab.

\subsection{4. Exercise}\label{exercise}

\subsubsection{4.1. Patient Background:}\label{exercise-background}

A 41-year-old man was admitted to a hospital 6 days after the onset of disease. He reported fever, chest tightness, unproductive cough, pain and weakness. Preliminary investigations excluded the presence of influenza virus, \emph{Chlamydia pneumoniae}, \emph{Mycoplasma pneumoniae}, and other common respiratory pathogens. After 3 days of treatment the patient was admitted to the intensive care unit, and 6 days following admission the patient was transferred to another hospital.

To further investigate the cause of illness, a sample of bronchoalveolar lavage fluid (BALF) was collected from the patient and metatranscriptomic sequencing was performed (that is, the RNA from the sample was sequenced). In this lab, you will examine the metatranscriptomic data using a number of bioinformatics methods and tools to attempt to identify the cause of the illness.

\emph{Note: The patient information and data was derived from a real study (shown at the end of the lab).}

\subsubsection{4.2. Overview}\label{exercise-overview}

We will proceed through the following steps to attempt to diagnose the situation.

\begin{itemize}
\tightlist
\item
  Trim and clean sequence reads using \texttt{fastp}
\item
  Filter host (human) reads with \texttt{kat}
\item
  Run Kraken2 with a bacterial and viral database to look at the taxonomic makeup of the reads.
\item
  Assemble the metatranscriptome with \texttt{megahit}
\item
  Examine assembly quality using \texttt{quast} and possible pathogens with \texttt{blast}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Assembly-free approach}\label{assembly-free}

The first set of steps follows through an assembly-free approach where we will perform taxonomic classification of the reads without constructing a metagenomics assembly.

\subsubsection{Step 1: Examine the reads}\label{exercise-examine-reads}

Let's first take a moment to examine the reads from the metatranscrimptomic sequencing. Note that for metatranscriptomic sequencing, while we are sequencing the RNA, this was performed by first generating complementary DNA (cDNA) to the RNA and sequencing the cDNA. Hence you will see thymine (T) instead of uracil (U) in the sequence data.

The reads were generated from paired-end sequencing, which means that a particular fragment (of cDNA) was sequenced twice--once from either end (see the \href{https://www.illumina.com/science/technology/next-generation-sequencing/plan-experiments/paired-end-vs-single-read.html}{Illumina Paired vs.~Single-End Reads} for some additional details). These pairs of cDNA sequence reads are stored as separate files (named \texttt{emerging-pathogen-reads\_1.fastq.gz} and \texttt{emerging-pathogen-reads\_2.fastq.gz}). You can see each file by running \texttt{ls}:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ls}\NormalTok{ ../data}
\end{Highlighting}
\end{Shaded}

\textbf{Output}

\begin{verbatim}
emerging-pathogen-reads_1.fastq.gz  emerging-pathogen-reads_2.fastq.gz
\end{verbatim}

We can look at the contents of one of the files by running \texttt{less} (you can look at the other pair of reads too, but it will look very similar):

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{less}\NormalTok{ ../data/emerging{-}pathogen{-}reads\_1.fastq.gz}
\end{Highlighting}
\end{Shaded}

\textbf{Output}

\begin{verbatim}
@SRR10971381.5 5 length=151
NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
+
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
@SRR10971381.7 7 length=151
NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
+
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
@SRR10971381.33 33 length=115
NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN
+
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
@SRR10971381.56 56 length=151
CCCGTGTTCGATTGGCATTTCACCCCTATCCACAACTCATCCCAAAGCTTTTCAACGCTCACGAGTTCGGTCCTCCACACAATTTTACCTGTGCTTCAACCTGGCCATGGATAGATCACTACGGTTTCGGGTCTACTATTACTAACTGAAC
+
FFFFFFFAFFFFFFAFFFFFF6FFFFFFFFF/FFFFFFFFFFFF/FFFFFFFFFFFFFFFFFAFFFFFFFFFFFAFFFFF/FFAF/FAFFFFFFFFFAFFFF/FFFFFFFFFFF/F=FF/FFFFA6FAFFFFF//FFAFFFFFFAFFFFFF
\end{verbatim}

These reads are in the \href{https://en.wikipedia.org/wiki/FASTQ_format}{FASTQ}, which stores a single read as a block of 4 lines: \textbf{identifier}, \textbf{sequence}, \textbf{+ (separator)}, \textbf{quality scores}. In this file, we can see a lot of lines with \texttt{NNN...} for the sequence letters, which means that these portions of the read are not determined. We will remove some of these undetermined (and uninformative) reads in the next step.

\subsubsection{Step 2: Clean and examine quality of the reads}\label{exercise-quality-reads}

As we saw from looking at the data, reads that come directly off of a sequencer may be of variable quality which might impact the downstream analysis. We will use the software \href{https://github.com/OpenGene/fastp}{fastp} to both clean and trim reads (removing poor-quality reads or sequencing adapters) as well as examine the quality of the reads. To do this please run the following (the expected time of this command is shown as \texttt{\#\ Time:\ 30\ seconds}).

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Time: 30 seconds}
\ExtensionTok{fastp} \AttributeTok{{-}{-}detect\_adapter\_for\_pe} \AttributeTok{{-}{-}in1}\NormalTok{ ../data/emerging{-}pathogen{-}reads\_1.fastq.gz }\AttributeTok{{-}{-}in2}\NormalTok{ ../data/emerging{-}pathogen{-}reads\_2.fastq.gz }\AttributeTok{{-}{-}out1}\NormalTok{ cleaned\_1.fastq }\AttributeTok{{-}{-}out2}\NormalTok{ cleaned\_2.fastq}
\end{Highlighting}
\end{Shaded}

You should see the following as output:

\textbf{Output}

\begin{verbatim}
Detecting adapter sequence for read1...
No adapter detected for read1

Detecting adapter sequence for read2...
No adapter detected for read2
[...]
Insert size peak (evaluated by paired-end reads): 150

JSON report: fastp.json
HTML report: fastp.html

fastp --detect_adapter_for_pe --in1 ../data/emerging-pathogen-reads_1.fastq.gz --in2 ../data/emerging-pathogen-reads_2.fastq.gz --out1 cleaned_1.fastq --out2 cleaned_2.fastq
fastp v0.24.0, time used: 17 seconds
\end{verbatim}

\subsubsection{Examine output}\label{examine-output}

You should now be able to nagivate to \textless{} \url{http://xx.uhn-hpc.ca/module2_workspace/analysis}\textgreater{} and see some of the output files. In particular, you should be able to find \textbf{fastp.html}, which contains a report of the quality of the reads and how many were removed. Please take a look at this report now:

This should show an overview of the quality of the reads before and after filtering with \texttt{fastp}. Using this report, please answer the following questions.

\subsubsection{Step 2: Questions}\label{step-2-questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Looking at the \textbf{Filtering result} section, how many reads \textbf{passed filters}? How many were removed due to \textbf{low quality}? How many were removed due to \textbf{too many N}?
\item
  Looking at the \textbf{Adapters} section, were there many adapters that needed to be trimmed in this data?
\item
  Compare the \textbf{quality} and \textbf{base contents} plots \textbf{Before filtering} and \textbf{After filtering}? How do they differ?
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Step 3: Host read filtering}\label{exercise-host-filtering}

The next step is to remove any host reads (in this case Human reads) from our dataset as we are not focused on examining host reads. There are several different tools that can be used to filter out host reads such as Bowtie2 or KAT. In this demonstration, we have selected to run KAT followed by Kraken2, but you could likely accomplish something similar by using Bowtie2 followed by Kraken2.

Command documentation is available \href{http://kat.readthedocs.io/en/latest/using.html\#sequence-filtering}{here}

KAT works by breaking down each read into small fragements of length \emph{k}, k-mers, and compares them to a k-mer database of the human reference genome. Subsequently, the complete read is either assigned into a matched or unmatched (filtered) file if 10\% of the k-mers in the read have been found in the human database.

Let's run KAT now.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Time: 3 minutes}
\ExtensionTok{kat}\NormalTok{ filter seq }\AttributeTok{{-}t}\NormalTok{ 4 }\AttributeTok{{-}i} \AttributeTok{{-}o}\NormalTok{ filtered }\AttributeTok{{-}{-}seq}\NormalTok{ cleaned\_1.fastq }\AttributeTok{{-}{-}seq2}\NormalTok{ cleaned\_2.fastq \textasciitilde{}/CourseData/module2/db/kat\_db/human\_kmers.jf}
\end{Highlighting}
\end{Shaded}

The arguments for this command are:

\begin{itemize}
\tightlist
\item
  \texttt{filter\ seq}: Specifies that we are running a specific subcommand to \textbf{filter sequences}.
\item
  \texttt{-t\ 4}: The number of threads to use (we have 4 CPU cores on these machines so we are using 4 threads).
\item
  \texttt{-\/-seq\ -\/-seq2} arguments to provide corresponding forward and reverse fastq reads (the cleaned reads from \texttt{fastp})
\item
  \texttt{-i}: Inverts the filter, that is we wish to output sequences \textbf{not found} in the human kmer database to a file.
\item
  \texttt{-o\ filtered} Provide prefix for all files generated by the command. In our case, we will have two output files \textbf{filtered.in.R1.fastq} and \textbf{filetered.in.R2.fastq}.
\item
  \texttt{\textasciitilde{}/CourseData/module2/db/kat\_db/human\_kmers.jf} the human k-mer database we're using made with jellyfish.
\end{itemize}

As the command is running you should see the following output on your screen:

\textbf{Output}

\begin{verbatim}
Kmer Analysis Toolkit (KAT) V2.4.2

Running KAT in filter sequence mode
-----------------------------------

Loading hashes into memory... done.  Time taken: 40.7s

Filtering sequences ...
Processed 100000 pairs
Processed 200000 pairs
[...]
Finished filtering.  Time taken: 130.5s

Found 1127908 / 1306231 to keep

KAT filter seq completed.
Total runtime: 182.8s
\end{verbatim}

If the command was successful, your current directory should contain two new files:

\begin{itemize}
\tightlist
\item
  \texttt{filtered.in.R1.fastq}
\item
  \texttt{filtered.in.R2.fastq}
\end{itemize}

These are the set of reads minus any reads that matched the human genome. The message \texttt{Found\ 1127908\ /\ 1306231\ to\ keep} tells us how many read-pairs were kept (the number in the \texttt{filtered.in.*.fastq} files) vs.~the total number of read-pairs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Step 4: Classify reads using Kraken2 database}\label{exercise-classify-kraken}

Now that we have most, if not all, host reads filtered out, it's time to classify the remaining reads to identify the likely taxonomic category they belong to.

Database selection is one of the most crucial parts of running Kraken. One of the many factors that must be considered is the computational resources available. Our current AWS image for the course has only 16G of memory. A major disadvantage of Kraken2 is that it loads the entire database into memory. With the \href{https://benlangmead.github.io/aws-indexes/k2}{standard viral, bacterial, and archael database} on the order of 50 GB we would be unable to run the full database on the course machine. To help mitigate this, Kraken2 allows reduced databases to be constructed, which will still give reasonable results but we may end up with more reads unclassified or classified at a higher taxonomic rank. We have constructed our own smaller Kraken2 database using only bacterial, human, and viral data. We will be using this database.

Lets run the following command in our current directory to classify our reads against the Kraken2 database.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Time: 1 minute}
\ExtensionTok{kraken2} \AttributeTok{{-}{-}db}\NormalTok{ \textasciitilde{}/CourseData/module2/db/kraken2\_db }\AttributeTok{{-}{-}threads}\NormalTok{ 4 }\AttributeTok{{-}{-}paired} \AttributeTok{{-}{-}output}\NormalTok{ kraken\_out.txt }\AttributeTok{{-}{-}report}\NormalTok{ kraken\_report.txt }\AttributeTok{{-}{-}unclassified{-}out}\NormalTok{ kraken2\_unclassified\#.fastq filtered.in.R1.fastq filtered.in.R2.fastq}
\end{Highlighting}
\end{Shaded}

This should produce output similar to below:

\textbf{Output}

\begin{verbatim}
Loading database information... done.
1127908 sequences (315.54 Mbp) processed in 5.332s (12693.3 Kseq/m, 3551.00 Mbp/m).
  880599 sequences classified (78.07%)
  247309 sequences unclassified (21.93%)
\end{verbatim}

\subsubsection{\texorpdfstring{Examine \texttt{kraken\_report.txt}}{Examine kraken\_report.txt}}\label{examine-kraken_report.txt}

Let's examine the text-based report of Kraken2:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{less}\NormalTok{ kraken\_report.txt}
\end{Highlighting}
\end{Shaded}

This should produce output similar to the following:

\begin{verbatim}
 21.93  247309  247309  U       0       unclassified
 78.07  880599  30      R       1       root
 78.01  879899  124     R1      131567    cellular organisms
 76.37  861411  19285   D       2           Bacteria
 56.53  637572  2       D1      1783270       FCB group
 56.53  637558  1571    D2      68336           Bacteroidetes/Chlorobi group
 56.39  635982  1901    P       976               Bacteroidetes
 55.10  621496  35      C       200643              Bacteroidia
 55.09  621417  19584   O       171549                Bacteroidales
 53.18  599872  2464    F       171552                  Prevotellaceae
 52.96  597396  397538  G       838                       Prevotella
  4.74  53473   53473   S       28137                       Prevotella veroralis
  4.34  48940   48940   S       1177574                     Prevotella jejuni
  2.56  28880   470     G1      2638335                     unclassified Prevotella
\end{verbatim}

This will show the top taxonomic ranks (right-most column) as well as the percent and number of reads that fall into these categories (left-most columns). For example:

\begin{itemize}
\tightlist
\item
  The very first row \texttt{21.93\ \ 247309\ \ 247309\ \ U\ \ \ \ \ \ \ 0\ \ \ \ \ \ \ unclassified} shows us that \textbf{247309 (21.93\%)} of the reads processed by Kraken2 are unclassified (remember we only used a database containing bacterial, viral, and human representatives).
\item
  The 4th line \texttt{76.37\ \ 861411\ \ 19285\ \ \ D\ \ \ \ \ \ \ 2\ \ \ \ \ \ \ \ \ \ \ Bacteria} tells us that \textbf{861411 (76.37\%)} of our reads fall into the \textbf{Bacteria} domain (the \texttt{D} in the fourth column is the taxonomic rank, \textbf{\texttt{D}omain}). The number \texttt{19285} tells us that \texttt{19285} of the reads are assigned directly to the \textbf{Bacteria} domain but cannot be assigned to any lower taxonomic rank (they match with too many diverse types of bacteria).
\end{itemize}

More details about how to read this report can be found at \url{https://github.com/DerrickWood/kraken2/wiki/Manual\#sample-report-output-format}. In the next step we will represent this data visually as a multi-layered pie chart.

\subsubsection{\texorpdfstring{Examine \texttt{kraken\_out.txt}}{Examine kraken\_out.txt}}\label{examine-kraken_out.txt}

Let's also take a look at \texttt{kraken\_out.txt}. This file contains the kraken2 results, but divided up into a classification for every read.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{column} \AttributeTok{{-}s}\StringTok{$\textquotesingle{}}\DataTypeTok{\textbackslash{}t}\StringTok{\textquotesingle{}} \AttributeTok{{-}t}\NormalTok{ kraken\_out.txt }\KeywordTok{|} \FunctionTok{less} \AttributeTok{{-}S}
\end{Highlighting}
\end{Shaded}

\texttt{column} formats a text file (\texttt{kraken\_out.txt}) into multiple columns according to a tab delimiter character (flag \texttt{-s\textquotesingle{}\$\textbackslash{}t\textquotesingle{}}) and produces a table (flag \texttt{-t}).

\textbf{Output}

\begin{verbatim}
C       SRR10971381.56  29465   151|151 0:40 909932:2 0:8 909932:2 0:20 29465:4 0:7 178327>
C       SRR10971381.97  838     122|122 0:44 2:5 0:23 838:1 0:10 838:2 0:3 |:| 0:3 838:2 0>
C       SRR10971381.126 9606    109|109 0:2 9606:5 0:7 9606:1 0:12 9606:1 0:47 |:| 0:47 96>
C       SRR10971381.135 838     151|151 0:95 838:3 0:19 |:| 0:15 838:1 0:12 838:5 0:6 838:>
C       SRR10971381.219 1177574 151|151 0:11 838:3 0:5 838:2 0:9 838:5 0:11 838:1 976:5 83>
C       SRR10971381.223 838     151|151 0:117 |:| 0:61 838:4 0:40 838:1 0:11
[...]
\end{verbatim}

This shows us a taxonomic classification for every read (one read per line). For example:

\begin{itemize}
\tightlist
\item
  On the first line, \texttt{C\ \ SRR10971381.56\ \ \ \ \ \ \ \ 29465} tells us that this read with identifier \texttt{SRR10971381.56} is classified \texttt{C} (matches to something in the Kraken2 database) and matches to the taxonomic category \texttt{29465}, which is the NCBI taxonomy identifer. In this case \texttt{29465} corresponds to the \href{https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&id=29465&lvl=3&lin=f&keep=1&srchmode=1&unlock}{\emph{Veillonella}}.
\end{itemize}

More information on interpreting this file can be found at \url{https://github.com/DerrickWood/kraken2/wiki/Manual\#standard-kraken-output-format}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Step 5: Generate an interactive html-based report using Pavian}\label{exercise-pavian}

Instead of reading text-based files like above, we can visualize this information using \href{https://fbreitwieser.shinyapps.io/pavian/}{Pavian}, which can be used to construct an interactive summary and visualization of metagenomics data. Pavian supports a number of metagenomics analysis software outputs, including Kraken/Kraken2. To visualize the Kraken2 output we just generated, we can upload the \texttt{kraken\_report.txt} file to the web application. Please do this now using the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Download the \texttt{kraken\_report.txt} to your local machine from \url{http://xx.uhn-hpc.ca/module2_workspace/analysis} (you can right-click and select \textbf{Save as\ldots{}} on the file).
\item
  Visit the \href{https://fbreitwieser.shinyapps.io/pavian/}{Pavian} website and click on \textbf{Upload files \textgreater{} Browse\ldots{}} and select the file \texttt{kraken\_report.txt} we just downloaded.
\item
  Select \textbf{Generate HTML report \ldots{}} to generate the Pavian report.
\item
  Open the generated report HTML file in your web browser.
\end{enumerate}

If all the steps are completed successfully then the report you should see should look like the following:

\subsubsection{Step 5: Questions}\label{step-5-questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the percentages of \textbf{Unclassified}, \textbf{Microbial}, \textbf{Bacterial}, \textbf{Viral}, \textbf{Fungal}, and \textbf{Protozoan} reads in this dataset?
\item
  Scroll down to the \textbf{Classification results} section of the report and flip through the \textbf{Bacteria}, \textbf{Viruses}, and \textbf{Eukaryotes} tabs. What is the top organism in each of these three categories and how many reads?
\item
  This data was derived from RNA (instead of DNA) and some viruses are RNA-based. If we focus in on the \textbf{Viruses} category, is there anything here that could be consistent with the patient's symptoms?
\item
  Given the results of Pavian, can you form a hypothesis as to the cause of the patient's symptoms?
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Assembly-based approach}\label{assembly-based}

\subsubsection{Step 6: Metatranscriptomic assembly}\label{exercise-metatranscriptomic-assembly}

In order to investigate the data further we will assemble the metatranscriptome using the software \href{https://github.com/voutcn/megahit}{MEGAHIT}. What this will do is integrate all the read data together to attempt to produce the longest set of contiguous sequences possible (contigs). To do this please run the following:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Time: 6 minutes}
\ExtensionTok{megahit} \AttributeTok{{-}t}\NormalTok{ 4 }\AttributeTok{{-}1}\NormalTok{ filtered.in.R1.fastq }\AttributeTok{{-}2}\NormalTok{ filtered.in.R2.fastq }\AttributeTok{{-}o}\NormalTok{ megahit\_out}
\end{Highlighting}
\end{Shaded}

If everything is working you should expect to see the following as output:

\textbf{Output}

\begin{verbatim}
2025-11-19 15:45:30 - MEGAHIT v1.2.9
2025-11-19 15:45:30 - Using megahit_core with POPCNT and BMI2 support
2025-11-19 15:45:30 - Convert reads to binary library
2025-11-19 15:45:31 - b'INFO  sequence/io/sequence_lib.cpp  :   75 - Lib 0 (/media/cbwdata/workspace/module2_workspace/analysis/filtered.in.R1.fastq,/media/cbwdata/workspace/module2_workspace/analysis/filtered.in.R2.fastq): pe, 2255816 reads, 151 max length'
2025-11-19 15:45:32 - b'INFO  utils/utils.h                 :  152 - Real: 1.3294\tuser: 1.2124\tsys: 0.3011\tmaxrss: 166748'
2025-11-19 15:45:32 - k-max reset to: 141
2025-11-19 15:45:32 - Start assembly. Number of CPU threads 4
2025-11-19 15:45:32 - k list: 21,29,39,59,79,99,119,141
2025-11-19 15:45:32 - Memory used: 14741121024
2025-11-19 15:45:32 - Extract solid (k+1)-mers for k = 21

[...]

2025-11-19 15:49:08 - Assemble contigs from SdBG for k = 141
2025-11-19 15:49:09 - Merging to output final contigs
2025-11-19 15:49:09 - 3112 contigs, total 1536607 bp, min 203 bp, max 29867 bp, avg 493 bp, N50 463 bp
2025-11-19 15:49:09 - ALL DONE. Time elapsed: 218.408143 seconds
\end{verbatim}

Once everything is completed, you will have a directory \texttt{megahit\_out/} with the output. Let's take a look at this now:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ls}\NormalTok{ megahit\_out/}
\end{Highlighting}
\end{Shaded}

\textbf{Output}

\begin{verbatim}
checkpoints.txt  done  final.contigs.fa  intermediate_contigs  log  options.json
\end{verbatim}

It's specifically the \textbf{final.contigs.fa} file that contains our metatranscriptome assembly. This will contain the largest \emph{contiguous} sequences MEGAHIT was able to construct from the sequence reads. We can look at the contents with the command \texttt{head} (\texttt{head} prints the first 10 lines of a file):

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{ megahit\_out/final.contigs.fa}
\end{Highlighting}
\end{Shaded}

\textbf{Output}

\begin{verbatim}
>k141_0 flag=1 multi=3.0000 len=312
ATACTGATCTTAGAAAGCTTAGATTTCATCTTTTCAATTGGTGTATCGAATTTAGATACAAATTTAGCTAAGGATTTAGACATTTCAGCTTTATCTACAGTAGAGTATACTTTAATATCTTGAAGTACACCAGTTACTTTAGACTTAATCAAAATTTTACCCAAATCATTAACTAGATCTTTAGAATCAGAATTCTTTTCTACCATTTTAGCGATGATATCTGTTGCATCTTGATCTTCAAATGAAGATCTATATGACATGATAGTTTGACCTTCTTGTAGTTGAGATCCAACTTCTAAACATTCGATGTCT
>k141_785 flag=1 multi=6.0000 len=355
TATAGAGATAGGTGATAAGGTTTTCCTAGGCGCACAATCAGGTGTACCTGGAAGTTTGAAGGCTAATCAACAACTGATTGGTACACCTCCAATGGAGCAACGCTCTTACTTTAAGTCGCAAGCCATCTTCCGCAGACTGCCTGAGATGTACAAGCAACTAAGCGACTTGCAGAAAGAAATAGACCAATTGAAAAAGAACAGATAACATTCATGGATACAATAAAACAGAAGACATTAAAAGGGAGTTTCTCGCTCTTTGGCAAGGGACTTCACACAGGTTTGAGTCTTACCGTGACGTTTAATCCGGCTCCAGAGAATACTGGCTATAAGATACAACGTATAGATCTTGAGGGGC
[...]
\end{verbatim}

It can be a bit difficult to get an overall idea of what is in this file, so in the next step we will use the software \href{http://quast.sourceforge.net/quast}{Quast} to summarize the assembly information.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Step 7: Evaluate assembly with Quast}\label{exercise-evaluate-assembly}

\href{http://quast.sourceforge.net/quast}{Quast} can be used to provide summary statistics on the output of assembly software. Quast will take as input an assembled genome or metagenome (a FASTA file of different sequences) and will produce HTML and PDF reports. We will run Quast on our data by running the following command:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Time: 2 seconds}
\ExtensionTok{quast} \AttributeTok{{-}t}\NormalTok{ 4 megahit\_out/final.contigs.fa}
\end{Highlighting}
\end{Shaded}

You should expect to see the following as output:

\textbf{Output}

\begin{verbatim}
/home/ubuntu/.conda/envs/module2-emerging-pathogen/bin/quast -t 4 megahit_out/final.contigs.fa

Version: 5.3.0

System information:
  OS: Linux-6.14.0-1016-aws-x86_64-with-glibc2.39 (linux_64)
  Python version: 3.9.23
  CPUs number: 4

Started: 2025-11-19 15:50:27

[...]

Finished: 2025-11-19 15:50:28
Elapsed time: 0:00:01.273336
NOTICEs: 1; WARNINGs: 0; non-fatal ERRORs: 0

Thank you for using QUAST!
\end{verbatim}

Quast writes it's output to a directory \texttt{quast\_results/}, which includes HTML and PDF reports. We can view this using a web browser by navigating to \url{http://xx.uhn-hpc.ca/module2_workspace/analysis/} and clicking on \textbf{quast\_results} then \textbf{latest} then \textbf{icarus.html}. From here, click on \textbf{Contig size viewer}. You should see the following:

This shows the length of each contig in the \texttt{megahit\_out/final.contigs.fa} file, sorted by size.

(If it is having issues you can also see the needed information in the \texttt{pdf} report)

\subsubsection{Step 7: Questions}\label{step-7-questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the length of the largest contig in the genome? How does it compare to the length of the 2nd and 3rd largest contigs?
\item
  Given that this is RNASeq data (i.e., sequences derived from RNA), what is the most common type of RNA you should expect to find? What are the approximate lengths of these RNA fragments? Is the largest contig an outlier (i.e., is it much longer than you would expect)?
\item
  Is there another type of source for this RNA fragment that could explain it's length? Possibly a \href{https://en.wikipedia.org/wiki/Coronavirus\#Genome}{Virus}?
\item
  Also try looking at the QUAST report (\url{http://xx.uhn-hpc.ca/module2_workspace/analysis/quast_results/latest/} then clicking on \textbf{report.html}). How many contigs \textgreater= 1000 bp are there compared to the number \textless{} 1000 bp?
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{Step 8: Use BLAST to look for existing organisms}\label{exercise-blast}

In order to get a better handle on what the identity of the largest contigs could be, let's use \href{https://blast.ncbi.nlm.nih.gov/Blast.cgi}{BLAST} to compare to a database of existing viruses. Please run the following:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Time: 1 second}
\ExtensionTok{seqkit}\NormalTok{ sort }\AttributeTok{{-}{-}by{-}length} \AttributeTok{{-}{-}reverse}\NormalTok{ megahit\_out/final.contigs.fa }\KeywordTok{|} \ExtensionTok{seqkit}\NormalTok{ head }\AttributeTok{{-}n}\NormalTok{ 50 }\OperatorTok{\textgreater{}}\NormalTok{ contigs{-}50.fa}
\ExtensionTok{blastn} \AttributeTok{{-}db}\NormalTok{ \textasciitilde{}/CourseData/module2/db/blast\_db/ref\_viruses\_rep\_genomes\_modified }\AttributeTok{{-}query}\NormalTok{ contigs{-}50.fa }\AttributeTok{{-}html} \AttributeTok{{-}out}\NormalTok{ blast\_results.html}
\end{Highlighting}
\end{Shaded}

As output you should see something like (\texttt{blastn} won't print any output):

\textbf{Output}

\begin{verbatim}
[INFO] read sequences ...
[INFO] 3112 sequences loaded
[INFO] sorting ...
[INFO] output ...
\end{verbatim}

Here, we first use \href{https://bioinf.shenwei.me/seqkit/}{seqkit} to sort all contigs by length with the largest ones first (\texttt{seqkit\ sort\ -\/-by-length\ -\/-reverse\ ...}) and we then extract only the top \textbf{50} longest contigs (\texttt{seqkit\ head\ -n\ 50}) and write these to a file \textbf{contigs-50.fa} (\texttt{\textgreater{}\ contigs-50.fa}).

\emph{Note that the pipe \texttt{\textbar{}} character will take the output of one command (\texttt{seqkit\ sort\ -\/-by-length\ ...}, which sorts sequences in the file by length) and forward it into the input of another command (\texttt{seqkit\ head\ -n\ 50}, which takes only the first 50 sequences from the file). The greater-than symbol \texttt{\textgreater{}} takes the output of one command \texttt{seqkit\ head\ ...} and writes it to a file (named \texttt{contigs-50.fa}).}

The next command will run \href{https://blast.ncbi.nlm.nih.gov/Blast.cgi}{BLAST} on these top 50 longest contigs using a pre-computed database of viral genomes (\texttt{blastn\ -db\ \textasciitilde{}/CourseData/module2/db/blast\_db/ref\_viruses\_rep\_genomes\_modified\ -query\ contigs-50.fa\ ...}). The \texttt{-html\ -out\ blast\_results.html} tells BLAST to write its results as an HTML file.

To view these results, please browse to \url{http://xx.uhn-hpc.ca/module2_workspace/analysis/blast_results.html} to view the ouptut \texttt{blast\_results.html} file. This should look something like below:

\subsubsection{Step 8: Questions}\label{step-8-questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the closest match for the longest contig you find in your data? What is the percent identify for this match (the value Z in \texttt{Identities\ =\ X/Y\ (Z\%)}). Recall that if a pathogen is an emerging/novel pathogen then you may not get a perfect match to any existing organisms.
\item
  Using the BLAST report alongside all other information we've gathered, what can you say about what pathogen may be causing the patient's symptoms?
\item
  It can be difficult to examine all the contigs/BLAST matches at once with the standard BLAST report (which shows the full alignment). We can modify the BLAST command to output a tab-separated file, with one BLAST HSP (a high-scoring segment pair) per line. To do this please run the following:

  \textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{blastn} \AttributeTok{{-}db}\NormalTok{ \textasciitilde{}/CourseData/module2/db/blast\_db/ref\_viruses\_rep\_genomes\_modified }\AttributeTok{{-}query}\NormalTok{ contigs{-}50.fa }\AttributeTok{{-}outfmt} \StringTok{\textquotesingle{}7 qseqid length slen pident sseqid stitle\textquotesingle{}} \AttributeTok{{-}out}\NormalTok{ blast\_report.tsv}
\end{Highlighting}
\end{Shaded}

  This should construct a tabular BLAST report with the columns labeled like \texttt{query\ id,\ alignment\ length,\ subject\ length,\ \%\ identity,\ subject\ id,\ subject\ title}. Taking a look at the file \texttt{blast\_report.tsv}, what are all the different BLAST matches you can find (the different values for \texttt{subject\ title})? How do they compare in terms of \texttt{\%\ identity} and \texttt{alignment\ length} (in general, higher values for both of these should be better matches)?
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{5. Final words}\label{final}

Congratulations, you've finished this lab. As a final check on your results, you can use \href{https://blast.ncbi.nlm.nih.gov/Blast.cgi}{NCBI's online tool} to perform a BLAST on our top 50 contigs to see what matches to the contigs.

The source of the data and patient background information can be found at \url{https://doi.org/10.1038/s41586-020-2008-3} (\textbf{clicking this link will reveal what the illness is}). The only modification made to the original metatranscriptomic reads was to reduce them to 10\% of the orginal file size.

While we used \textbf{MEGAHIT} to perform the assembly, there are a number of other more recent assemblers that may be useful. In particular, the \href{https://cab.spbu.ru/software/spades/}{SPAdes} suite of tools (such as \href{https://doi.org/10.1093/bioinformatics/btaa490}{metaviralspades} or \href{https://doi.org/10.1093/gigascience/giz100}{rnaspades}) may be useful to look into for this sort of data analysis.

As a final note, NCBI also performs taxonomic analysis using their own software and you can actually view these using Krona directly from NCBI. Please click \href{https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR10971381}{here} and go to the \emph{Analysis} tab for NCBI's taxonomic analysis of this sequence data (\textbf{clicking this link will reveal what the illness is}).

\chapter{Module 3 Pathogen Typing}\label{module-3-pathogen-typing}

\section{Lecture}\label{lecture-2}

\section{Lab}\label{lab-2}

\subsection{Introduction}\label{introduction}

The scope of this practical session is to perform a core-genome multi-locus sequence typing (cgMLST) analysis, a genome-based molecular typing method widely adopted for genomic surveillance of bacterial species.

Due to the widespread adoption of high-throughput whole-genome sequencing (WGS) and the advancement of this technology, it has been incorporated in many surveillance programs such as \href{https://www.canada.ca/en/public-health/programs/pulsenet-canada.html}{PulseNet Canada to monitor foodborne outbreaks}.

This practical is split into two sessions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  cgMLST allele schema creation all through to allele calling, which will be conducted on your terminal to generate necessary input files for the next session
\item
  Clustering of cgMLST data from chewBBACA and visualization in RStudio
\end{enumerate}

In this exercise, you will begin by generating cgMLST allele calls for a collection of 200 \emph{Staphylococcus aureus} genomes (50 complete and 150 draft genomes) from NCBI RefSeq and BVBRC.

\subsection{PART ONE: chewBBACA Objective}\label{part-one-chewbbaca-objective}

\begin{itemize}
\tightlist
\item
  To create wgMLST and cgMLST schema for a collection of 200 MRSA \emph{Staphylococcus aureus} genomes (50 complete genomes and 150 draft genomes) from NCBI RefSeq and BVBRC
\end{itemize}

\subsubsection{Activating Conda environment}\label{activating-conda-environment}

Before we begin, the software we need to conduct our analysis have already been installed on your instance using \href{https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html}{Conda}. However, we need to activate this environment to use the necessary tools.

\begin{verbatim}
# Activate conda environment to use chewBBACA and Prodigal
conda activate chewie
\end{verbatim}

\subsubsection{Create training file using Prodigal}\label{create-training-file-using-prodigal}

ChewBBACA requires a training file to predict genes and proteins for our species of interest. For this purpose, we are going to use the \emph{S. aureus} USA300 reference genome to generate the training file.

\begin{verbatim}
prodigal -i USA300ref.fasta -t Staphylococcus_aureus.trn -p single
\end{verbatim}

\subsubsection{Create wgMLST schema using the complete genomes from NCBI}\label{create-wgmlst-schema-using-the-complete-genomes-from-ncbi}

Now that we have our training file, we will start by creating a whole-genome multi-locus sequence typing (wgMLST) schema based on the complete 50 \emph{S. aureus} genomes from NCBI RefSeq

\begin{verbatim}
chewBBACA.py CreateSchema -i genomes/NCBI-RefSeq-50 -o mrsa_schema --ptf Staphylococcus_aureus.trn --cpu 4
\end{verbatim}

This will use the prodigal training file to identify CDS from the complete set of S. aureus genomes in NCBI and save them in the mrsa\_schema folder

\begin{quote}
The schema seed will be found in that folder with 3,389 identified loci.
\end{quote}

\subsubsection{Allele Calling}\label{allele-calling}

Next is to perform allele calling with the wgMLST schema that we created in the previous step.

This will determine the allelic profiles of the strains we are analyzing by identifying novel alleles, which will be added to the schema.

\begin{verbatim}
chewBBACA.py AlleleCall -i genomes/NCBI-RefSeq-50 -g mrsa_schema/schema_seed -o wgMLST_50 --cpu 4
\end{verbatim}

\begin{quote}
Using a BLAST score ratio (BSR) of 0.6 and clustering similarity of 0.2, 13,762 novel alleles were identified, bringing the number of alleles in the schema to 17,151.
\end{quote}

\subsubsection{Paralog Detection}\label{paralog-detection}

If you noticed from the previous results, 13 paralogs were identified. You can also find them in the \texttt{paralogous\_counts.tsv} in the wgMLST\_50 folder. It is important to remove these paralogs from the schema due to their uncertainty in allele calls. To do this run the following

\begin{verbatim}
chewBBACA.py RemoveGenes -i wgMLST_50/results_alleles.tsv -g wgMLST_50/paralogous_counts.tsv -o wgMLST_50/results_alleles_NoParalogs.tsv
\end{verbatim}

\begin{quote}
By doing this, the new allelic profile without the paralogs are now saved in \texttt{results\_alleles\_NoParalogs.tsv}, which encompasses 3,376 loci
\end{quote}

\subsubsection{cgMLST schema analysis}\label{cgmlst-schema-analysis}

After some QC, we can now determine how many loci are present in the core genome based on the allele calling results. This is based on a given threshold of loci presence in the genomes we analyzed. The ExtractCgMLST module uses 95\%, 99\%, and 100\% to determine the set of loci in the core genome.

\begin{verbatim}
chewBBACA.py ExtractCgMLST -i wgMLST_50/results_alleles_NoParalogs.tsv -o wgMLST_50/cgMLST
\end{verbatim}

If you look in the cgMLST folder, there is an interactive HTML line plot showing the cgMLST per threshold relative to the number of loci detected.

\begin{quote}
From the plot, how many loci are present in the core genome at 95\%? We would use this threshold to account for loci that might not be identified due to sequencing coverage and assembly issues.
\end{quote}

\subsubsection{cgMLST assignment for 150 genomes}\label{cgmlst-assignment-for-150-genomes}

These are 150 MRSA genomes with good quality from Australia pulled from BVBRC with associated metadata. Mislaballed species have been excluded.

\begin{quote}
Using the 1,999 loci found in 95\% core genomes, we will perfome Allele assignment on the 150 genomes
\end{quote}

\begin{verbatim}
chewBBACA.py AlleleCall -i genomes/BVBRC-150 -g mrsa_schema/schema_seed --gl wgMLST_50/cgMLST/cgMLSTschema95.txt -o BVBRC-150_results --cpu 4
\end{verbatim}

\begin{quote}
At the end of the analysis, this added 13,244 novel alleles to the schema with no paralogs
\end{quote}

\subsubsection{Convert Allele Calls}\label{convert-allele-calls}

To convert the allelic profiles into a suitable format that can be parsed into other tools, we need to convert the non-integers such as INF, ASM, PLOT3, and PLOT5 into integars

\begin{verbatim}
chewBBACA.py ExtractCgMLST -i BVBRC-150_results/results_alleles.tsv -o BVBRC-150_results/cgMLST-150
\end{verbatim}

\subsection{PART TWO: cgMLST clustering Objective}\label{part-two-cgmlst-clustering-objective}

The purpose of this lab session is to take the cgMLST allele assignment that we have previously generated with chewBBACA and cluster them using Hamming distance matrix and allele thresholds. In addition, we would visualize our dataset using a dendrogram and annotate it with our cluster threshold and associated metadata.

The overall goal is to infer genetic relatedness in our dataset by leveraging computational tools to investigate potential outbreaks and their potential sources.

\subsubsection{Getting Started}\label{getting-started}

We will begin by installing the necessary packages and helper script that we need to analyze our data.

Every time you begin a new R session, you must reload all the packages and scripts!

\begin{verbatim}
# install packages requiring BiocManager 

if (!requireNamespace('BiocManager', quietly = TRUE))
    install.packages('BiocManager')

if (!requireNamespace('ComplexHeatmap', quietly = TRUE))
    BiocManager::install('ComplexHeatmap')

if (!requireNamespace('treedataverse', quietly = TRUE))
    BiocManager::install("YuLab-SMU/treedataverse")

# install other packages

required_packages <- c("tidyverse", "data.table", "BiocManager", "plotly", "ggnewscale", "circlize",
                      "randomcoloR", "phangorn", "knitr", "purrr", "scales", "remotes","reactable",
                      "ggtreeExtra")

not_installed <- required_packages[!required_packages %in% installed.packages()[,"Package"]]

if (length(not_installed) > 0) {
  install.packages(not_installed, quiet = TRUE)
}


# load packages 

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(data.table))
suppressPackageStartupMessages(library(treedataverse))
suppressPackageStartupMessages(library(plotly))
suppressPackageStartupMessages(library(ggnewscale))
suppressPackageStartupMessages(library(ComplexHeatmap))
suppressPackageStartupMessages(library(circlize))
suppressPackageStartupMessages(library(randomcoloR))
suppressPackageStartupMessages(library(RColorBrewer))
suppressPackageStartupMessages(library(phangorn))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(purrr))
suppressPackageStartupMessages(library(scales))
suppressPackageStartupMessages(library(reactable))
suppressPackageStartupMessages(library(ggnewscale))
suppressPackageStartupMessages(library(ggtreeExtra))

## source helper R scripts
source("src/ggtree_helper.R")
source("src/cluster_count_helper.R")
source("src/cluster_helper.R")
source("src/cgmlst_helper.R")
\end{verbatim}

Next read the cgMLST data from chewBBACA and the metadata into memory

\begin{verbatim}
# read cgMLST and metadata
aus_mrsa_cgMLST <- read.delim("BVBRC-150_results/cgMLST-150/cgMLST95.tsv", sep = "\t")

mrsa.data <- read.csv("MRSA_AUS_metadata.csv", header = F)
\end{verbatim}

Having loaded our files, we need to make some adjustments to our metadata to make it suitable for our analysis.

\begin{quote}
first, we are going to change the header,
then we are going to create a new column for the host to exclude the taxon ID
\end{quote}

\begin{verbatim}
# change the header of the metadata file
new.header <- mrsa.data[2,]
aus_mrsa_metadata <- mrsa.data[-c(1,2),]
colnames(aus_mrsa_metadata) <- new.header

head(aus_mrsa_metadata)

aus_mrsa_metadata_reordered <- aus_mrsa_metadata %>% 
  select(6, everything()) # move 6th column to the first position

head(aus_mrsa_metadata_reordered)

# Create a new Host column with just the host name
aus_mrsa_metadata_reordered <- aus_mrsa_metadata_reordered %>%
  mutate(Host = str_remove(`host (common name)`, "\\s*\\[.*\\]"))
\end{verbatim}

\begin{quote}
Can you spot the differences between the two metadata? Which one do you think we are going to use?
\end{quote}

\subsubsection{Calculating the Hamming Distance}\label{calculating-the-hamming-distance}

The traditional approach for cgMLST analysis is based on computing pairwise distances between allele profiles as a proxy for the underlying genetic similarity between two isolates. Below, you are introduced to a distance metric called the \textbf{Hamming distance}, which is based on computing the number of differences between a pair of character vectors. The Hamming distance is useful for comparing profiles where a majority of characters are defined, such as profiles comprising core loci.

Given two character vectors of equal lengths, the Hamming distance is the total number of positions in which the two vectors are \ul{different:}

Profile A: \texttt{{[}\ 0\ ,\ 2\ ,\ 0\ ,\ 5\ ,\ 5\ ,\ 0\ ,\ 0\ ,\ 0\ ,\ 0\ {]}}

Profile B: \texttt{{[}\ 0\ ,\ 1\ ,\ 0\ ,\ 4\ ,\ 3\ ,\ 0\ ,\ 0\ ,\ 0\ ,\ 0\ {]}}

~~A != B: \texttt{{[}\ 0\ ,\ 1\ ,\ 0\ ,\ 1\ ,\ 1\ ,\ 0\ ,\ 0\ ,\ 0\ ,\ 0\ {]}} ~~

Hamming distance = \texttt{sum(\ A\ !=\ B\ )} = 3

\begin{quote}
In phylogenetic analysis, distance-based approaches are rather flexible in the sense that they can be constructed from any measure that estimates genetic similarity through the direct comparison of data points. In cgMLST, we compare allele profiles. In Mash, we compare k-mer profiles.
\end{quote}

In the context of two cgMLST profiles, the Hamming distance is calculated based on the number of allele differences across all loci as a proportion of total number of loci evaluated.

Hamming distances will be computed in an \emph{all vs.~all} fashion to generate a pairwise distance matrix that will subsequently serve as the input for distance-based tree-building algorithms. Run the following code chunk:

\begin{verbatim}
# use hamming helper function to compute pairwise Hamming distance of cgMLST data 

# compute Hamming distance 
# PATIENCE!!! this step might take several minutes depending on the size of the dataset

mrsa.dist_mat <- aus_mrsa_cgMLST %>% 
  column_to_rownames("FILE") %>% 
  t() %>% 
  hamming()

# the dimension should be symmetric and should be the size of dataset (i.e. number of QC-passed genomes)

dim(mrsa.dist_mat)

\end{verbatim}

\subsubsection{Hierarchical Clustering of cgMLST profiles by Hamming distance}\label{hierarchical-clustering-of-cgmlst-profiles-by-hamming-distance}

In this section we will be performing hierarchical clustering of the Hamming distance matrix using the \href{https://en.wikipedia.org/wiki/UPGMA}{Unweighted Pair Group Method with Arithmetic Mean} (i.e.~\textbf{UPGMA}) algorithm.

Let's cluster the Hamming distance matrix by running the code chunk below:

\begin{verbatim}
#  compute hierarchical clustering with hclust function and complete linkage method

mrsa.hc <- mrsa.dist_mat%>%
        as.dist() %>%
         hclust(method = "complete")

# reorder distance matrix according to the hc order 

mrsa.dist_mat <- mrsa.dist_mat[mrsa.hc$order, mrsa.hc$order]

# write reordered  distance matrix to files 
dir.create("output/clusters", recursive = T)
write.table(mrsa.dist_mat, file = "output/clusters/dist_mat_ordered_cgmlst.tsv",
             quote = F, row.names = F, sep = "\t")
             
\end{verbatim}

\subsubsection{Cluster extraction at multiple distance thresholds}\label{cluster-extraction-at-multiple-distance-thresholds}

Identifying clusters of genomes sharing highly similar cgMLST profiles through the application of distance thresholds is a common practice in genomic surveillance and epidemiological investigations. These genomic clusters can become ``analytical units'' that can be tracked across space and time:

\begin{itemize}
\tightlist
\item
  The detection of a novel genomic cluster comprising isolates from multiple human clinical cases can signal the emergence of an outbreak, thus requiring a public health response in order to contain further cases.\\
\item
  An examination of the evolving genomic cluster over time can provide important epidemiological insights on outbreak progression.
\item
  The co-clustering of outbreak isolates with isolates from food/environmental sources can assist epidemiologists investigating an outbreak by linking the outbreak isolates to isolates from possible sources/reservoirs of the pathogen.
\end{itemize}

Identifying cluster membership for every isolate in the dataset at multiple distance thresholds gives us the analytical flexibility to define suitable thresholds for analyzing the pathogen in question. From a practical perspective, a threshold that is too fine-grained will yield a large proportion of the dataset in singleton clusters, making it difficult to link outbreak isolates to one another and to potential outbreak sources. Conversely, using a threshold that is not fine-grained enough will fail to fully exploit the discriminatory power of genomic data, grouping outbreak and non-outbreak isolates indiscriminately.

\begin{quote}
Adjusting similarity thresholds for cluster membership can be used to tweak the granularity of clusters: higher distance threshold produce larger clusters whereas lower distance thresholds will produce smaller clusters. A threshold of 0 will generate clusters with \textbf{identical} profiles.
\end{quote}

\textbf{For membership at all possible distance thresholds}, run the following code chunk:

\begin{verbatim}

# Define genomic cluster membership at all possible distance thresholds

mrsa.cgmlst_loci = (ncol(aus_mrsa_cgMLST)-1) ## maximum distance 
interval = 1  ##  edit interval of interest; currently set to 1 then change to 10 to see how many thresholds are generated

threshld <-  seq(0, mrsa.cgmlst_loci,interval)


# extract cluster membership across multiple thresholds
cluster_group <- map(threshld, function(x) {
    mrsa.hc %>% 
    cutree(h = x) %>% 
    as.factor()
})

# create name for each threshold
names(cluster_group) <- paste0("Threshold_", threshld)


# print clustering results table, no duplicated names are allowed
 (
all.clusters <- data.frame(cluster_group ) %>%
    rownames_to_column("FILE")
 )  

# reactable(clusters_all)

# write file of genomic cluster memberships at multiple thresholds
write.table(all.clusters, file = "output/clusters/clusters_all.tsv",
            quote = F, row.names = F, sep = "\t")
\end{verbatim}

\begin{quote}
\textbf{Questions:} How many distinct thresholds are theoretically possible for this dataset? How would you determine the maximum number of allele differences across all genomes in the dataset based on the table above? \emph{hint: at some threshold, all genomes collapse to the same cluster\ldots{}}
\end{quote}

\textbf{For membership at select distance thresholds}, run the following code chunk:

\begin{verbatim}
# Define genomic cluster membership at user-defined distance thresholds

# Can either specify specific thresholds as a numeric vector i.e. c(0, 5, 10, 15) 
# If you're feeling lazy, you can also specify using c(seq(5, 100, 5)), which stands for "go from threshold 5 to 100 in
# increments of 5". You can also string multiple notations together within the same numerical vector.  

threshld <-c(0, seq(5, 100, 5), seq(200, mrsa.cgmlst_loci, 100))   # currently reads as "use threshold 0, 
                                                                # then from 5 to 100 in increments of 5
                                                                # then from 200 to maximum threshold 
                                                                # in increments of 100"

# extract cluster membership across multiple thresholds
 cluster_group <- map(threshld, function(x) {
     mrsa.hc %>%
     cutree(h = x) %>%
     as.factor()
 })

# create name for each threshold
 names(cluster_group) <- paste0("Threshold_", threshld)

# print clustering results table, no duplicated names are allowed
 (
   new_defined_clusters <- data.frame(cluster_group ) %>%
     rownames_to_column("FILE")
   )

# write file
 write.table(new_defined_clusters, file = "output/clusters/new_defined_clusters.tsv",
             quote = F, row.names = F, sep = "\t")

\end{verbatim}

\begin{quote}
\textbf{Questions:} How many user-defined thresholds have we generated in the settings provided here? Which cluster does genome ``1280\_21738'' belong to at a threshold of 25 allele differences? How would you determine how many different clusters were generated at each particular threshold?
\end{quote}

\subsubsection{Distance matrix visulization via ordered heatmap}\label{distance-matrix-visulization-via-ordered-heatmap}

A very useful visualization when dealing with distance matrices involves performing clustering, arranging the entries of the matrix based on the clustering order, and displaying the similarity as a heatmap. This produces a heatmap with a distinct 45 degree axis in which clusters of profiles with significant similarity representing possible clades/lineages can be plainly seen as ``pockets of heat''. In this section of the lab, we will visualize the distance matrix using the \texttt{ComplexHeatmap} package and we will be comparing these clades to the MLST genotype information on the isolates by overlaying MLST information on the heatmap, which will allow us to examine whether the organism's MLST is concordant with putative lineages defined by cgMLST similarity as displayed on the clustered heatmap.

Let's run the code chunk below:

\begin{verbatim}
# create column annotations for heatmap
# to display clade information
set.seed(123)

htmap_annot <-  aus_mrsa_metadata_reordered$genotype
names(htmap_annot) <-aus_mrsa_metadata$`BV-BRC genome ID`

htmap_annot <- htmap_annot[order(factor(names(htmap_annot),
                                            levels = rownames(mrsa.dist_mat)))]

# create heatmap
mrsa.dist_mat %>% 
  Heatmap(
    name = "cgMLST\nDistance",
    show_row_names = F, # do not display row labels
    show_column_names = F, # do not display column labels
    # use custom color gradient
    col = colorRamp2(
      c(min(mrsa.dist_mat), mean(mrsa.dist_mat), max(mrsa.dist_mat)),
      c("#f76c6a", "#eebd32", "#7ece97")
    ),
    # add column annotation to show MLST info overlaid on the clustered heatmap
    top_annotation = HeatmapAnnotation(
      MLST= htmap_annot,
      col = list(
      MLST = structure(brewer.pal(length(unique(htmap_annot)), "Set3"),
                                             names = unique(htmap_annot))
      )
    )
    
  )
\end{verbatim}

\subsubsection{Annotated Dendrograms and cluster evaluation}\label{annotated-dendrograms-and-cluster-evaluation}

Here you will convert hierarchical clustering result (mrsa.hc) into a dendrogram using the \texttt{as.phylo()} function from the ape package. To visualize the resulting dendrogram, we will use the R package \texttt{ggtree}, which offers an extensive suite of functions to manipulate, visualize, and annotate tree-like data structures. In this section, you will be introduced to some of the different visual capabilities of \texttt{ggtree} and we will progressively update the same tree with several layers of visual annotations based on available metadata.

\begin{quote}
Note that there is a massive amount of information in the internet dedicated to \texttt{ggtree} and all of its capabilities for advanced visualizations. Here we will barely scratch the surface.
\end{quote}

\subsubsection{Circular vs.~Rectangular dendrograms for simple tree visualization}\label{circular-vs.-rectangular-dendrograms-for-simple-tree-visualization}

Radial vs.~Rectangular dendrograms are different ways of visualizing a tree. Radial trees are capable of displaying a lot of simple data in a smaller footprint. In contrast, rectangular trees can display more complex data in a visualization that is easy on the eyes. Rectangular trees rendered as pdf format allow you to really zoom into sub-branches of the tree in greater detail when viewed in a pdf reader or in a browser window. This is essential when you're dealing with larger datasets comprising hundreds of isolates such as the one we are dealing with here.

Run the following code chunks to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Plot a circular tree of the entire dataset with the tree tips colored by MLST information. \emph{You can assign a different metadata field to the \texttt{color\_var} variable to update the mapping of the color aesthetics in the tree. For example setting \texttt{color\_var\ =\ "Host"} will color the tree tips by source of isolate}.
\item
  Plot the same tree as a rectangular tree.
\end{enumerate}

\begin{verbatim}
# convert hierarchical clustering to a dendrogram 
set.seed(123)
mrsa.cg_tree <- as.phylo(mrsa.hc)

# also, write out the dendrogram to a Newick tree file, can be imported into ITOL or other tree visualization software for manual annotation
dir.create("output/trees", recursive = T)
write.tree(mrsa.cg_tree, file = "output/trees/tree_complete_linkage.newick")

# visualization with a color variable using R ggtree 
color_var <- "genotype" # editable variable, currently set to "MLST".

## filter out unknown serotypes to define the number of colors needed for remaining serotypes 
n_colors <- length(unique(pull(aus_mrsa_metadata_reordered, !!sym(color_var))))

## create circular dendrogram with variable-colored tippoints
cg_tree_cir <- mrsa.cg_tree %>% 
  ggtree(layout='circular', # set tree shape 
         size = 1 # branch width
  )%<+% aus_mrsa_metadata_reordered +
  geom_tippoint(aes(color = as.factor(!!sym(color_var))),
                size = 2,na.rm=TRUE) +
  guides(color = guide_legend(title = "MLST", override.aes = list(size = 3) ) ) +
  scale_color_manual(values = distinctColorPalette(n_colors),na.value = "grey") 

cg_tree_cir


# Create rectangular tree with MLST tip points and text tip labels

# create tip labels 
aus_mrsa_metadata_reordered <- aus_mrsa_metadata_reordered %>%
            mutate(tip_lab = paste0(Host,"/ ",sample_collection_date))

## plot    
set.seed(123)
  mrsa.cg_tree %>% 
  ggtree(layout= "rectangular")%<+% aus_mrsa_metadata_reordered +
  geom_tippoint(aes(color = as.factor(!!sym(color_var))),
                size = 3,na.rm=TRUE) +
  geom_tiplab(aes(label = tip_lab),  
              offset = 5,
              align = TRUE,
              linetype = NULL,
              size = 3) +theme_tree2()+
  geom_treescale(y = 130, x = 0.2) +
  guides(color = guide_legend(title = "MLST", override.aes = list(size = 3) ) ) +
  scale_color_manual(values = distinctColorPalette(n_colors))
  
 # We save to pdf format
 ggsave(file = "output/trees/clade_subtree_rec.pdf", height = 45, width = 40) 
 
\end{verbatim}

\begin{quote}
\textbf{Questions:} Between ST22 and ST93, which one shows more heterogeneity based on the cgMLST data?
\end{quote}

\subsubsection{Superimposing genomic cluster information onto dendrograms}\label{superimposing-genomic-cluster-information-onto-dendrograms}

Let's now superimpose the genomic cluster information on the previous dendrograms (Radial \& Rectangular) to examine whether the above code chunk for extracting cluster memberships at various thresholds has generated sensible cluster assignments. Run the code chunk below to insert text labels that span across tree tips assigned to the same clusters at \ul{a specified threshold}. Interchange your threshold to 50 and 15 and see how it affects your cluster outcome.

\begin{quote}
You can edit the \texttt{target\_threshold} variable to examine how cluster membership changes in response to clustering distance cutoffs.
\end{quote}

Run the code chunk below for the radial tree:

\begin{verbatim}
# Radial tree visualization

# assign all clusters to clusters for visualization  
mrsa.clusters <- all.clusters 

target_threshold <- 25 # Editable variable,  currently set to a threshold of 25.

aus_mrsa_metadata_reordered <- aus_mrsa_metadata_reordered %>%
             select(-tip_lab)

# variable to subset clusters
target_variable <- paste0("Threshold_", target_threshold)

# create cluster group list object
cluster_grp <- mrsa.clusters %>% 
  select(FILE, target_variable) %>%
  group_by(!!sym(target_variable)) %>% 
  {setNames(group_split(.), group_keys(.)[[1]])} %>% 
  map(~pull(., FILE))
# sequester singleton clusters
cluster_grp <- cluster_grp[which(map_dbl(cluster_grp, ~length(.)) > 10)]

# create clade group list object
meta2 <- aus_mrsa_metadata_reordered %>%
         filter(genotype != "unknown")
Clade_grp <- meta2 %>% 
  select(`BV-BRC genome ID`, genotype) %>% 
  split(f = as.factor(.$genotype)) %>% 
  map(~pull(., `BV-BRC genome ID`))

# add cluster memberships and clade information to tree object
mrsa.cg_tree <- groupOTU(mrsa.cg_tree, cluster_grp, 'Clusters')
mrsa.cg_tree <- groupOTU(mrsa.cg_tree, Clade_grp, 'MLST')

# plot core genome tree where colored blocks = clusters and text annotations = clades
valid_clade_grp <- Clade_grp %>%
  keep(~all(.x %in% mrsa.cg_tree$tip.label)) %>%
  keep(~length(.x) > 1)  # Need at least 2 tips for MRCA

valid_cluster_grp <- cluster_grp %>%
  keep(~all(.x %in% mrsa.cg_tree$tip.label)) %>%
  keep(~length(.x) > 1)

mlst.t25 <- mrsa.cg_tree %>% 
  ggtree(layout='circular', # Editable tree shape, currently set to circular?
         size = 1 # branch width
  ) +
 
  # add colored blocks to display clades
  geom_hilight(
    mapping = aes(
      node = node,
      fill = MLST,
      subset = node %in% map_dbl(
        valid_clade_grp,
        ~getMRCA(mrsa.cg_tree, .)
        )
      )
    ) +
  #add text annotations to display clusters
  geom_cladelab(
    mapping = aes(
      node = node,
      label = Clusters,
      subset = node %in% map_dbl(
        valid_cluster_grp,
        ~ getMRCA(mrsa.cg_tree, .)
      )
    ),
    horizontal=T,
    angle = 'auto',
    barsize = 0.75,
    offset = 50,
    offset.text = 50,
    align = T
  ) +
  # legend parameters
  guides(fill = guide_legend(
    nrow = 11,
    override.aes = list(alpha = 0.8)
    )
  ) +
  labs(fill = "MLST") +
  scale_fill_brewer(palette = "Paired")

mlst.t25
\end{verbatim}

\begin{verbatim}
final_plot <- mlst.t25 %<+% aus_mrsa_metadata_reordered +
  geom_tippoint(aes(color = Host), size =2) +
    scale_colour_manual(name = "Host", values = c("#008080","#ffa500","#00ff00","#0000ff","#ff1493"))

final_plot
\end{verbatim}

Here we can see a couple of things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The cgMLST cluster threshold of 25 allele differences has more discriminatory power than MLST for investigating putative outbreaks or inferring genetic similarity
\item
  Cluster 50 comprises of mostly horse isolates and two human isolates
\item
  We can infer the directionality of the outbreak in cluster 50 by looking at the sample collection dates and location if the data are present
\end{enumerate}

\begin{verbatim}
ggsave("Threshold25_clusters.pdf", plot = final_plot, height = 10, width = 8, device = "pdf")

\end{verbatim}

\textbf{Congratulations!} You have successfully completed Module 3.

\chapter{Module 4 Outbreak Analysis}\label{module-4-outbreak-analysis}

\emph{Author: Finlay Maguire}

\emph{Last Modified: 2025-11-11}

\section{Lecture}\label{lecture-3}

Link to PDF of slides from google

\section{Lab}\label{lab-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \hyperref[intro]{Background}
\item
  \hyperref[setup]{Setup}
\item
  \hyperref[cluster]{Cluster Identification}
\item
  \hyperref[snp]{SNP Analysis}
\item
  \hyperref[trans]{Transmission Inference}
\end{enumerate}

In this lab practical we will be using microbial genomes (and associated contextual metadata) to investigate a suspected nosocomial (hospital-acquired) outbreak of \emph{Methicillin resistance Staphylococcus aureus} (MRSA) in a UK-based Neonatal Intensive Care Unit (NICU). The data we are using comes from a classic study which demonstrated the utility of whole genome sequencing for investigating an MRSA outbreak within a Special Care Baby Unit (also known as a NICU) of the Cambridge University Hospitals NHS Foundation Trust (CUH).

\begin{quote}
Harris SR, Cartwright EJP, Trk ME, et al.~Whole-genome sequencing for analysis of an outbreak of methicillin-resistant Staphylococcus aureus: a descriptive study. \emph{Lancet Infect Dis} 2012 \href{http://dx.doi.org/10.1016/S1473-3099(12)70268-2}{10.1016/S1473-3099(12)70268-2}
\end{quote}

This practical will step you through the genomic components of a typical nosocomial bacterial outbreak investigation, specifically:

\begin{itemize}
\tightlist
\item
  Identifying potential outbreak clusters
\item
  Determining SNP distances
\item
  Inferring outbreak phylogenies
\item
  Performing transmission inference
\end{itemize}

\subsection{Background}\label{background}

\emph{Staphylococcus aureus} is a Gram-positive coccoidal bacteria that forms a normal commensal member of the microbiome in 20-40\% of people (\href{https://doi.org/10.1016/j.ijmm.2016.11.007}{Karsten, 2017}). However, when skin and mucosal barriers are disrupted (e.g., through medical procedures such as catheterisation, tracheal tubes, surgeries) \emph{S. aureus} can cause opportunistic invasive infections (\href{https://doi.org/10.1038/nrdp.2018.33}{Lee, 2018}).

Since the 1960s, variants have emerged and spread globally that are resistant to the majority of -lactam antibiotics through the independent acquisitions of the staphylococcal cassette chromosome mec (SCCmec) (\href{https://doi.org/10.1128/AAC.00579-09}{IWG-SSC, 2009}. This combination of commensal carriage, antimicrobial resistance, and healthcare-associated infection opportunities (along with additional virulence factors) have led to MRSA becoming a leading cause of hospital-associated mortality and morbidity globally (\href{https://doi.org/10.1016/s0140-6736(24)01867-1}{GBD, 2024}). Therefore, tracking and preventing MRSA outbreaks is a major priority for infection prevention and control (IPAC or IPC) within hospital settings especially within the NICU due to the vulnerability of immunocompromised preterm infants and the high frequency of invasive procedures.
This has led to adoption of genomic epidemiological approaches to detect MRSA outbreaks and track transmission links not apparent from traditional approaches alone (\href{https://doi.org/10.1099/mgen.0.001235}{Blane, 2024}). These sort of outbreak investigations are generally led by the infection prevention \& control (IPAC or IPC) team in a clinical setting or by the relevant public health team depending on the suspected infection source, geographic scale, and regional resource availability (e.g., Public Health Agency of Canada/Canadian Food Inspection Agency or provincial bodies like Public Health Ontario).

Within the SCBU/NICU of Cambridge University Hopitals NHS Trust (and most hospitals) all inpatients are screened for MRSA carriage upon admission and once per week thereafter using culture-based or nucleic-acid based test. Specific IPAC policies and guidelines will determine criteria for initiating an outbreak investigation (e.g., a certain number of positive screens within a specific time and/or location).

Routine surveillance has identified that 3 infants within the SCBU/NICU (P11-P13) are positive for MRSA carriage at the same time. Isolates from these 3 infants have also been shown to have the same pattern of antibiotic resistances. The IPAC team has therefore been activated to further investigate this as a suspected outbreak in the SCBU/NICU at CUH.
They have performed a systematic review of all MRSA isolates from the SCBU/NICU over the preceding 6-12 months and identified a series of overlapping MRSA carriages with this same set of resistances. Due to multi-week gaps in the positive SCBU/NICU isolates they have also collected potentially matching MRSA isolates from parents and the wider hospital/community. To gain better insight into this potential outbreak and identify which of these isolates are linked by direct transmission chains, CUH have sequenced these isolates using 150bp paired-end reads via an Illumina MiSeq platform.

You are a clinical bioinformatician who has been asked to support the investigation and analyses these genomes

\subsection{Set-Up}\label{set-up}

This lab practical will involve running the following software:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/tseemann/mlst}{mlst}
\item
  \href{https://github.com/bacpop/PopPUNK}{poppunk}
\item
  \href{https://github.com/bacpop/ska.rust}{ska}
\item
  \href{https://github.com/nickjcroucher/gubbins}{gubbins}
\item
  \href{https://github.com/iqtree/iqtree3}{IQTree}
\item
  \href{https://graphsnp.fordelab.com/}{GraphSNP}
\end{itemize}

Typically, most of these analyses would be run using a workflow such as \href{https://bactopia.github.io/latest/}{bactopia} that you are confident will reproducibly generate validated and verified outputs (ideally as part of your institution's ISO15189 accreditation or equivalent). We will run the tools directly today so you get insight into what these workflows are actually doing!

\subsubsection{Data}\label{data}

Using ssh connect to the provided analysis server instance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ssh} \AttributeTok{{-}i}\NormalTok{ YOUR\_KEY.pem YOUR\_USER@XX.uhn{-}hpc.ca}
\end{Highlighting}
\end{Shaded}

Now create a folder in your \texttt{\textasciitilde{}/workspace} and copy/link over the files you will need:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ \textasciitilde{}/workspace/module4}
\BuiltInTok{cd}\NormalTok{ \textasciitilde{}/workspace/module4}
\FunctionTok{cp} \AttributeTok{{-}r}\NormalTok{ \textasciitilde{}/CourseData/module4/contextual\_data.csv  \textasciitilde{}/CourseData/module4/SASCBU26\_reference.fna .}
\FunctionTok{ln} \AttributeTok{{-}s}\NormalTok{ \textasciitilde{}/CourseData/module4/assemblies .}
\end{Highlighting}
\end{Shaded}

Specific collection dates were not available so have been inferred based on relative sampling gaps

When you are finished with these steps you should be inside your work directory \texttt{\textasciitilde{}/workspace/module4}. You can verify this by running the command \texttt{pwd}.
s
\textbf{Output after running \texttt{pwd}}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{\textasciitilde{}/workspace/module4}
\end{Highlighting}
\end{Shaded}

You now see an \texttt{assemblies/} folder in the current directory along with a \texttt{contextual\_metadata.tsv} (all the contextual data needed for this analysis) and \texttt{SASCBU26\_reference.fna} (the reference genome we will use later):

\textbf{Output after running \texttt{ls}}s

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{SASCBU26\_reference.fna}\NormalTok{  assemblies  contextual\_data.csv}
\end{Highlighting}
\end{Shaded}

\subsubsection{Activate environment}\label{activate-environment-1}

Next we will activate the pre-installed \href{https://docs.conda.io/en/latest/}{conda} environment, which will have all the tools needed by this tutorial pre-installed. To do this please run the following:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate outbreak}
\end{Highlighting}
\end{Shaded}

You should see the command-prompt (where you type commands) switch to include \texttt{(outbreak)} at the beginning, showing you are inside this environment. You should also be able to run the \texttt{mlst} command like \texttt{mlst\ -\/-version} and see output:

\textbf{Output after running \texttt{mlst\ -\/-version}}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{mlst}\NormalTok{ 2.23.0}
\end{Highlighting}
\end{Shaded}

\subsubsection{Find your IP address}\label{find-your-ip-address}

Similar to yesterday, we will want to either use the assigned hostname (e.g., xx.uhn-hpc.ca where xx is your instance number) or find the IP address of your machine on AWS so we can access some files from your machine on the web browser. To find your IP address you can run:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{curl}\NormalTok{ http://checkip.amazonaws.com}
\end{Highlighting}
\end{Shaded}

This should print a number like XX.XX.XX.XX. Once you have your address, try going to \url{http://xx.uhn-hpc.ca} or \url{http://IP-ADDRESS} and clicking the link for \textbf{module4}. This page will be referred to later to easily download/view some of our output files.

\subsection{Cluster Identification}\label{cluster-identification}

All isolates that are putatively linked to the outbreak have been done so on the basis of time, location, and phenotype (specifically antibiotic susceptibilities). However, just because 2 isolates have the same resistance pattern does not mean they are closely related.
Although this is a relatively small set of isolates we are often evaluating very large numbers of genomes for their potential connection to an outbreak.
Additionally, most downstream outbreak analyses (such as transmission inference) perform best when applied to as little diversity as possible (i.e., just genomes from a single outbreak event).

This means our first task is to identify and group which isolates are actually connect

are likely to be linked using the sort of typing methods you encountered in the previous lab practical.
This helps us to eliminate unrelated genomes and determine whether 1 or more outbreaks are likely to be taking place.

To make things faster for this practical, we have provided pre-inferred genome assemblies (generated using \href{https://github.com/tseemann/shovill}{shovill} with the \href{https://github.com/ncbi/SKESA}{skesa} assembler).

You can infer the 7-gene MLST using the \href{https://pubmlst.org/organisms/staphylococcus-aureus}{pubMLST \emph{Staphylococcus aureus} scheme} by running the \texttt{mlst} tool on each of the genome assemblies.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ outbreak\_cluster\_identification}\KeywordTok{;} \BuiltInTok{cd}\NormalTok{ outbreak\_cluster\_identification}
\ExtensionTok{mlst} \AttributeTok{{-}{-}scheme}\NormalTok{ saureus ../assemblies/}\PreprocessorTok{*}\NormalTok{.fa }\OperatorTok{\textgreater{}}\NormalTok{ mlst.tsv}
\end{Highlighting}
\end{Shaded}

Now download the \texttt{mlst.tsv} output file (using either \texttt{scp} or the IP address listed above) and open it in your tabular data tool of choice (e.g., pandas/python, R, excel).

\textbf{Based on these results, which genomes do you suspect may not be part of this outbreak?}

\begin{itemize}
\tightlist
\item
  P27 \& P28 have different MLSTs (ST1 and ST8) to any other genome so unlikely to be linked to an outbreak.
\item
  P29 \& P34-38 (with P37 a partial match) could be an ST22 outbreak.
\item
  P30-33 could be an ST772 outbreak.
\item
  All remaining isolates belong to a large ST2731 set
\end{itemize}

\textbf{Why might genomes with a different MLST still be closely related or those with the same MLST be relatively unrelated?}

MLST are only 7 genes so a small amount of mutation (or conservation!) in just these genes can lead to a totally different MLST - schemes are designed to try and be robust but are mutation is a random process with high variance!

We could solve this challenge using a more fine-grained scheme like cg/wgMLST (as discussed in the previous module) but we are going to use \texttt{poppunk} in this lab.

\texttt{poppunk} is a rapid k-mer based genome clustering tool that groups genomes together based on both their core and accessory genome distances. This allows differential clustering of similar genomes which have acquired novel plasmids and so on.

The developers of poppunk provide some pre-computed databases (although it is relatively easy to create your own) for different species which are available \href{https://www.bacpop.org/poppunk-databases/}{here}.

We have already downloaded a reference database for you, so all you have to do is prepare a 2-column file \texttt{genomes.txt} which has a series of names in the first column and the path to the related genome assembly in the other.
We can do this using a quick bash one-line loop:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ ../assemblies/}\PreprocessorTok{*}\NormalTok{.fa}\KeywordTok{;} \ControlFlowTok{do} \VariableTok{isolate}\OperatorTok{=}\VariableTok{$(}\BuiltInTok{echo} \VariableTok{$i} \KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}d} \StringTok{\textquotesingle{}/\textquotesingle{}} \AttributeTok{{-}f3} \KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}d} \StringTok{\textquotesingle{}.\textquotesingle{}} \AttributeTok{{-}f1}\VariableTok{)}\KeywordTok{;} \BuiltInTok{echo} \AttributeTok{{-}e} \StringTok{"}\VariableTok{$isolate}\StringTok{\textbackslash{}t}\VariableTok{$i}\StringTok{"} \OperatorTok{\textgreater{}\textgreater{}}\NormalTok{ genomes.txt}\KeywordTok{;} \ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

Then we can run the following to get poppunk cluster assignments:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{poppunk\_assign} \AttributeTok{{-}{-}db}\NormalTok{ \textasciitilde{}/CourseData/module4/staphylococcus\_aureus\_v1\_full }\AttributeTok{{-}{-}query}\NormalTok{ genomes.txt }\AttributeTok{{-}{-}output}\NormalTok{ poppunk}
\end{Highlighting}
\end{Shaded}

Now download/inspect \texttt{poppunk/poppunk\_clusters.csv} file

\textbf{Which genomes does this analysis suggest should be excluded? Do they match the MLST results?}

P27-28 are assigned to cluster 1 and P30-33 are assigned cluster 12. Remaining samples are assigned to cluster 3.

\begin{itemize}
\tightlist
\item
  P27 and P28 were different singleton STs for MLST that share a few alleles so supports eliminating them
\item
  P30-33 were ST772 so poppunk and MLST agree and support eliminating these samples (although they could be their own distinct outbreak)
\item
  ST22 and ST2731 isolates were both assigned to cluster 3. If we look carefully at the MLST profile for these STs in \texttt{mlst.tsv} we can see that these STs only differ by a single allele (arcC) so could be linked in the main outbreak.
\end{itemize}

We can't be sure about cluster 3 without further analysis but we can drop cluster 1 and 12 for now.

\subsection{SNP Analysis}\label{snp-analysis}

Now that we have eliminated some isolates that are unlikely to be connected to our main outbreak we can do a deeper analysis of the evolutionary relationships between our isolates. For this we are going to perform a phylogenetic analysis. There are several way this could be done:

\begin{itemize}
\tightlist
\item
  Inferring and aligning the core genome of our isolates by running a tool like \texttt{panaroo} on the genome assembly annotation files (generated with \texttt{prokka} or \texttt{bakta}).
\item
  Mapping individual reads against a reference genome with a tool like \texttt{snippy}.
\item
  Mapping SNPs between genomes using \texttt{ska} (then using a reference to order these SNPs)
\end{itemize}

\textbf{For SNP analyses, why might we want use a reference derived from an outbreak isolate instead of standard species reference genome?}

We maximise the number of detectable SNPs by using a reference as close as possible to our other genomes

For this lab, we are going to do the last option using a complete high quality reference genome generated from this outbreak subsequent to the original manuscript: SASCBU26

First let's generate the input file for SKA using the poppunk results

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{cd}\NormalTok{ \textasciitilde{}/workspace/module4}\KeywordTok{;} \FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ ska\_phylogeny}\KeywordTok{;} \BuiltInTok{cd}\NormalTok{ ska\_phylogeny}
\FunctionTok{paste} \OperatorTok{\textless{}(}\FunctionTok{grep} \StringTok{"3$"}\NormalTok{ ../outbreak\_cluster\_identification/poppunk/poppunk\_clusters.csv }\KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}d,} \AttributeTok{{-}f1}\OperatorTok{)} \OperatorTok{\textless{}(}\FunctionTok{grep} \StringTok{"3$"}\NormalTok{ ../outbreak\_cluster\_identification/poppunk/poppunk\_clusters.csv }\KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}d,} \AttributeTok{{-}f1} \KeywordTok{|} \FunctionTok{perl} \AttributeTok{{-}ne} \StringTok{\textquotesingle{}chomp; print "../assemblies/$\_.fa\textbackslash{}n"\textquotesingle{}} \OperatorTok{)} \OperatorTok{\textgreater{}}\NormalTok{ ska\_input}
\BuiltInTok{echo} \AttributeTok{{-}e} \StringTok{"SASCBU26\textbackslash{}t../SASCBU26\_reference.fna"} \OperatorTok{\textgreater{}\textgreater{}}\NormalTok{ ska\_input}
\end{Highlighting}
\end{Shaded}

Now we need to generate an SKA index:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{ska}\NormalTok{ build }\AttributeTok{{-}f}\NormalTok{ ska\_input }\AttributeTok{{-}k}\NormalTok{ 31 }\AttributeTok{{-}o}\NormalTok{ ska\_index }\AttributeTok{{-}{-}threads}\NormalTok{ 4}
\end{Highlighting}
\end{Shaded}

Then map these split k-mers against the reference (required to order them for the next step):

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{ska}\NormalTok{ map }\AttributeTok{{-}o}\NormalTok{ ska.aln }\AttributeTok{{-}{-}ambig{-}mask}\NormalTok{ ../SASCBU26\_reference.fna ska\_index.skf}
\end{Highlighting}
\end{Shaded}

This gives us an alignment but due the distorting effects of recombination we typically want to try and remove any potential recombinant sites from the alignment. There are several tools that can help us do this (\texttt{verticall}, \texttt{ClonalFrameML}, \texttt{gubbins}) and today we are going to use \texttt{gubbins}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir} \AttributeTok{{-}p}\NormalTok{ gubbins}
\ExtensionTok{run\_gubbins.py} \AttributeTok{{-}{-}prefix}\NormalTok{ gubbins/gubbins ska.aln}
\ExtensionTok{mask\_gubbins\_aln.py} \AttributeTok{{-}{-}aln}\NormalTok{ ska.aln }\AttributeTok{{-}{-}gff}\NormalTok{ gubbins/gubbins.recombination\_predictions.gff }\AttributeTok{{-}{-}out}\NormalTok{ gubbins.masked.aln}
\end{Highlighting}
\end{Shaded}

Now that we have a masked alignment \texttt{gubbins.masked.aln} we can analyse the SNP network using \href{https://graphsnp.fordelab.com/}{GraphSNP} to further refine our potential outbreak samples. First, generate a SNP distance matrix using \texttt{snp-dists}:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{snp{-}dists}\NormalTok{ gubbins.masked.aln }\KeywordTok{|} \FunctionTok{sed} \StringTok{\textquotesingle{}s/\textbackslash{}t/,/g\textquotesingle{}} \OperatorTok{\textgreater{}}\NormalTok{ raw\_outbreak\_matrix.csv}
\end{Highlighting}
\end{Shaded}

Now download the \texttt{raw\_outbreak\_matrix.csv}, navigate to \href{https://graphsnp.fordelab.com/}{GraphSNP} in your browser, and upload the \texttt{raw\_outbreak\_matrix.csv} as your Alignment/matrix.

\pandocbounded{\includegraphics[keepaspectratio]{content-files/graphsnp_upload.jpg}}

Then click on ``Graph'' at the top of the page (may be hidden behind 3 horizontal lines on smaller screens), increase your ``Cutoff number'' to 50, and then hit the ``Create Graph'' button.

\pandocbounded{\includegraphics[keepaspectratio]{content-files/graphsnp_cluster.jpg}}

This will create a minimum spanning tree grouped using a 50 SNP cut-off.

\textbf{Which genomes do you think you can eliminate from being part of these immediate outbreak cluster?}

Based on the graph, isolates P29 \& P34-P38 can be eliminated. These were the ST22 isolates identified by \texttt{mlst} that poppunk convinced us to look at a bit more closely. We can see the closest is only \textasciitilde80 SNPs from our outbreak cluster.

Let's also look at these isolates using a traditional phylogenetic approach. Go back to the server and run the following command to build a maximum-likelihood tree using \texttt{iqtree}:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{iqtree} \AttributeTok{{-}s}\NormalTok{ gubbins.masked.aln}
\end{Highlighting}
\end{Shaded}

This will generate a lot of outputs but if you download the newick formatted phylogeny \texttt{gubbins.masked.aln.treefile} and go to \href{https://microreact.org/upload}{microreact} we can visualise the tree.

Hit the ``Upload'' button and select your treefile.
\pandocbounded{\includegraphics[keepaspectratio]{content-files/microreact_upload.jpg}}

\pandocbounded{\includegraphics[keepaspectratio]{content-files/microreact_upload2.jpg}}

Then hit the settings slider icon at the top right

\pandocbounded{\includegraphics[keepaspectratio]{content-files/microreact_upload3.jpg}}

Finally, select radial/unrooted tree.

\pandocbounded{\includegraphics[keepaspectratio]{content-files/microreact_upload4.jpg}}

\textbf{Does the phylogeny show the same genomes as being outside the main outbreak cluster}

Yes, P37 \& P38 are close to the main outbreak cluster and could be included but for now we will exclude them.

Now we have refined our final set of suspected outbreak isolates we can create our final alignment of just these sequences.

\textbf{Create a SNP distance matrix and phylogeny for just the final outbreak isolates by repeating the above steps with a modified ska\_input (if you create a new folder you'll avoid getting mixed up with your new datas).}

A pre-computed distance matrix and phylogeny can be copied from \texttt{\textasciitilde{}/CourseData/module4/refined\_outbreak\_matrix.csv} and \texttt{\textasciitilde{}/CourseData/module4/refined.gubbins.masked.aln.treefile}

\#\#\# Transmission Inference

With our refined outbreak distance matrix and phylogeny we are going to perform some transmission analyses by combining these with some of our context data.

First we are going to use the \href{https://pmc.ncbi.nlm.nih.gov/articles/PMC3183872/}{SeqTrack} method as implemented in \href{https://graphsnp.fordelab.com/}{GraphSNP}.

To do this download \texttt{contextual\_metadata.csv} and your refined distance matrix (e.g., \texttt{refined\_outbreak\_matrix.csv} or whatever you have named it) and then navigate to \href{https://graphsnp.fordelab.com/}{GraphSNP}.\\
This time you must upload the new distance matrix under Alignment/matrix and the metadata table under Metadata.

\pandocbounded{\includegraphics[keepaspectratio]{content-files/graphsnp_upload.jpg}}

Then navigate to ``Graph'' but this time select analysis type ``transmission'' and click ``Create Graph''

\pandocbounded{\includegraphics[keepaspectratio]{content-files/graphsnp_transmission.jpg}}

\textbf{Which person is inferred to have infected the largest number of other patients?}

P8 or P4

\textbf{Which baby/babies was/were inferred to be infected by a parent?}

P13 and P15

Congratulations you've performed your first transmission analysis!

The next step would be to try and perform a more sophisticated transmission inference method using \texttt{TransPhylo}.
However, this would require us to infer a time-scaled phylogeny and you'll discover more about how to do that in the next module!

\chapter{Module 5 Phylodynamics}\label{module-5-phylodynamics}

\section{Lecture}\label{lecture-4}

\section{Lab}\label{lab-4}

\subsection{Table of contents}\label{table-of-contents}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \hyperref[intro-mod5]{Introduction}
\item
  \hyperref[software-mod5]{Software}\\
\item
  \hyperref[setup-mod5]{Exercise setup}
\item
  \hyperref[phylodyn-mod5]{Maximum likelihood phylodynamics analysis}
\item
  \hyperref[visualize-tree-mod5]{Visualizing the phylogeny in auspice}
\item
  \hyperref[delphy-mod5]{Delphy Bayesian phylodynamic analysis}
\item
  \hyperref[end-mod5]{End of lab}
\end{enumerate}

\subsection{1. Introduction}\label{introduction-1}

\subsubsection{1.1. Background}\label{background-1}

SARS-CoV-2 Pango lineage B.1.1.7 \href{https://www.nature.com/articles/s41564-020-0770-5}{(Rambaut et al.~2020)} was the first officially designated variant of concern (VOC) by the World Health Organization \href{https://www.nature.com/articles/s41564-021-00932-w}{(Konings et al.~2021)}, due to its increased transmissibility compared to other circulating lineages. Although B.1.1.7 was discovered in December 2020, it originated in the months preceding that. For a more thorough investigation into the circumstances of B.1.1.7 discovery and emergence, see \href{https://academic.oup.com/ve/article/8/2/veac080/6677185}{(Hill et al., 2022)} on ``The origins and molecular evolution of SARS-CoV-2 lineage B.1.1.7 in the UK''.

Evaluating when and where (and in what host, for multi-host pathogens) novel lineages or variants with increased transmissibility or virulence emerge is important towards improving our understanding of the conditions supporting the emergence of variants against which pharmaceutical (vaccines, therapies) or non-pharmaceutical interventions may be less effective.

In this tutorial, we will analyze early SARS-CoV-2 B.1.1.7 sequences and associated metadata from the first months following its detection to reconstruct its likely time and place of origin using phylodynamics and phylogeography, in both maximum likelihood and Bayesian statistical frameworks.

\subsubsection{1.2. Objectives of this analysis:}\label{objectives-of-this-analysis}

\begin{itemize}
\item
  Download and clean publicly available SARS-CoV-2 B.1.1.7 sequences
\item
  Remove sequences with incomplete sample dates and subsample fewer sequences (keeping earliest with higher probability)
\item
  Align sequences and filter out low-quality sequences
\item
  Summarize the spatiotemporal distribution of samples
\item
  Infer a maximum likelihood phylogenetic tree (divergence tree)
\item
  Infer a maximum likelihood time-scaled tree using a molecular clock model
\item
  Conduct ancestral state reconstruction to identify the most likely geographic origin of lineage B.1.1.7
\item
  Compare the maximum likelihood estimates to those obtained in a Bayesian framework using Delphy
\end{itemize}

\subsubsection{1.3. What statistical approach is best?}\label{what-statistical-approach-is-best}

Maximum likelihood and Bayesian approaches to phylodynamics and phylogeography differ in their assumptions, algorithms, computational speed, scalability, and interpretation.

For large scale analyses (\textgreater10,000 sequences), maximum likelihood pipelines implemented in augur, treetime, iqtree, and R package ape, are more feasible, while analyses of smaller datasets (up to \textasciitilde1000 sequences) can be done using Bayesian frameworks implemented in \href{https://delphy.fathom.info/}{Delphy} and \href{http://www.beast2.org/}{BEAST}, with the added benefits of incorporating uncertainty in the tree inference in parameter estimation and of specifying priors to limit the search space.

If you are interested in exploring more complex applications and methods for phylodynamic analyses, I recommend checking out resources from the \href{https://taming-the-beast.org/workshops/Taming-the-BEAST-Online/}{``Taming the BEAST'' workshops} that are held semi-regularly, as well as he tutorials shared by the \href{http://www.beast2.org/tutorials/}{BEAST Developers}. Paul Lewis' \href{http://phyloseminar.org/recorded.html}{phyloseminar.org} lectures on Bayesian phylogenetics (\href{https://www.youtube.com/watch?v=4PWlnNsfz90}{part 1} and \href{https://www.youtube.com/watch?v=TLtOS--YwkU}{part 2}) are also an excellent place to start to learn the basic theory of Bayesian methods.

\emph{Let's go.}

\subsection{2. List of software for tutorial}\label{list-of-software-for-tutorial}

\begin{itemize}
\tightlist
\item
  \href{https://tidyverse.org/packages/}{R package tidyverse}
\item
  \href{https://github.com/Bioconductor/Biostrings}{R package Biostrings}
\item
  \href{https://cran.r-project.org/web/packages/gtools/index.html}{R package gtools}
\item
  \href{https://cran.r-project.org/web/packages/RColorBrewer/index.html}{R package RColorBrewer}
\item
  \href{https://docs.nextstrain.org/projects/augur/en/stable/index.html}{augur}
\item
  \href{https://mafft.cbrc.jp/alignment/software/}{mafft}
\item
  \href{http://www.iqtree.org/}{iqtree}
\item
  \href{https://treetime.readthedocs.io/en/latest/}{treetime}
\item
  \href{https://auspice.us/}{Auspice}
\item
  \href{https://delphy.fathom.info/}{Delphy}
\end{itemize}

\subsection{3. Exercise setup}\label{exercise-setup}

\subsubsection{3.0. Data source}\label{data-source}

We will not spend time downloading the data, but rather will copy the datafiles as described in the subsequent section. However, it's important to understand how this was done for the sake of reproducibility and interpretation.

\subsubsection{Filters applied to download this dataset}\label{filters-applied-to-download-this-dataset}

SARS-CoV-2 B.1.1.7 (Alpha variant) data from \href{https://www.ncbi.nlm.nih.gov/labs/virus/vssi/\#/}{NCBI Virus}

\begin{itemize}
\tightlist
\item
  Pango lineage: B.1.1.7
\item
  Host: human
\item
  Dates:

  \begin{itemize}
  \tightlist
  \item
    Collection date: Sep 1 2020 to Jan 1 2021 (First few months of detection)
  \item
    Release date: Sep 1 2020 to Jan 1 2022 (allow one year delayed submission)
  \end{itemize}
\item
  Sequence Quality:

  \begin{itemize}
  \tightlist
  \item
    Nucleotide completeness: complete only
  \end{itemize}
\item
  Nucleotide \textgreater{} Download all results
\item
  Data type

  \begin{itemize}
  \tightlist
  \item
    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \tightlist
    \item
      sequence data = nucleotide; Fasta definition line: accession, country, collection date
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumi{\arabic{enumi}.}
    \setcounter{enumi}{1}
    \tightlist
    \item
      Results table in csv format; Results csv \textgreater{} Accession with version number
    \end{enumerate}
  \end{itemize}
\item
  N=15790 fasta sequences (b117\_seq.fasta) and metadata (b117\_seq.csv)
\end{itemize}

\textbf{Information to include when downloading metadata from NCBI:}
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/ncbi_down.png}}

\subsubsection{3.1. Copy data files}\label{copy-data-files-1}

To begin, we will copy over the downloaded files to \texttt{\textasciitilde{}/workspace}.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cp} \AttributeTok{{-}r}\NormalTok{ \textasciitilde{}/CourseData/module5 \textasciitilde{}/workspace/.}
\BuiltInTok{cd}\NormalTok{ \textasciitilde{}/workspace/module5}
\end{Highlighting}
\end{Shaded}

When you are finished with these steps you should be inside the directory \texttt{/home/ubuntu/workspace/module5}. You can verify this by running the command \texttt{pwd}.

\textbf{Output after running \texttt{pwd}}

\begin{verbatim}
/home/ubuntu/workspace/module5
\end{verbatim}

You should also see a directory \texttt{data/} in the current directory which contains all the input data. You can verify this by running \texttt{ls\ data/download}:

\textbf{Output after running \texttt{ls\ data/download}}

\begin{verbatim}
b117_seq.csv    b117_seq.fasta     reference_MN908947.3.fasta 
\end{verbatim}

It is also good to verify the data was copied fully. Run this code to count how many sequences are in the fasta file (each one starts with \textgreater):
\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grep} \AttributeTok{{-}c} \StringTok{"\^{}\textgreater{}"}\NormalTok{ data/download/b117\_seq.fasta }
\end{Highlighting}
\end{Shaded}

This should be 15790.

\subsubsection{3.2. Activate conda environment}\label{activate-conda-environment}

Next we will activate the \href{https://docs.conda.io/en/latest/}{conda} environment, which will have all the tools needed by this tutorial pre-installed.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{conda}\NormalTok{ activate sc2b117\_phylodyn}
\end{Highlighting}
\end{Shaded}

You should see the command-prompt (where you type commands) switch to include \texttt{(sc2b117\_phylodyn)} at the beginning, showing you are inside this environment. You should also be able to run the \texttt{augur} command like \texttt{augur\ -\/-version} and see output:

\textbf{Output after running \texttt{augur\ -\/-version}}

\begin{verbatim}
augur 32.0.0 
\end{verbatim}

\subsubsection{3.3. Find your IP address to setup an AWS instance}\label{find-your-ip-address-to-setup-an-aws-instance}

Similar to yesterday, we will want to either use the assigned hostname (e.g., xx.uhn-hpc.ca where xx is your instance number) or find the IP address of your machine on AWS so we can access some files from your machine on the web browser. To find your IP address you can run:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{curl}\NormalTok{ http://checkip.amazonaws.com}
\end{Highlighting}
\end{Shaded}

This should print a number like XX.XX.XX.XX. Once you have your address, try going to \url{http://xx.uhn-hpc.ca} and clicking the link for \textbf{module5}. This page will be referred to later to view some of our output files. In addition, the link \textbf{precomputed-analysis} will contain all the files we will generate during this lab (phylogenetic trees, etc).

\subsubsection{3.4. Inspect and clean data}\label{inspect-and-clean-data}

The first step of any phylogenetic analysis is to inspect and clean the dataset. Initial cleaning (before even looking at the sequence quality) steps include checking that accessions and sequence names are complete and unique, checking the completeness of sampling dates and locations (especially in phylodynamic analyses where these play a central role), checking the range of dates are realistic, and looking at how many geographic categories there are and whether some are more well-represented than others.

Run the following Rscript in the command line to de-duplicate seqeunces, remove incomplete dates, and subsample the data.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Rscript}\NormalTok{ scripts/dedup\_comp\_dates.R }\StringTok{"data/download/b117\_seq.csv"} \StringTok{"data/download/b117\_seq.fasta"}
\end{Highlighting}
\end{Shaded}

While this is running, let's have a look inside the R script. Open the .Rproj file in RStudio, then open scripts/dedup\_comp\_dates.R. Note that while we have called this script from the command line, you can also run it within Rstudio by manually specifying the arguments.

What is happening inside this script:

\begin{itemize}
\tightlist
\item
  De-duplicate identical sequences (taking earliest collection date of replicates)
\item
  Rename sequences without any spaces and add sequence name to metadata
\item
  Identify and remove incomplete collection date (yyyy or yyyy-mm)
\item
  Create a smaller subsampled dataset `complete\_sampUK' with equal number of non-UK and UK samples

  \begin{itemize}
  \tightlist
  \item
    \emph{NOTE: this is the dataset that we will be working with in this lab due to limited time.} In a more robust analysis, we would include more samples, repeat sampling with different seeds, and rather than sampling at random, we might consider sampling proportionally to geographies' diagnosed cases through time to more accurately reflect spatiotemporal dynamics
  \end{itemize}
\item
  Export fasta and csv files into data/complete subfolder
\end{itemize}

\begin{itemize}
\tightlist
\item
  How many sequences were downloaded?
\item
  How many sequences were there after de-duplicating?
\item
  After de-duplicating, how many had complete dates? How many were sampled (sampUK)?
\item
  How might you modify the subsampling scheme to reduce bias? to quantify uncertainty in the sampling process?
\end{itemize}

\begin{itemize}
\tightlist
\item
  n=15790
\item
  n=14774
\item
  n=10813; n=178
\item
  Proportional to case counts through time and space. Multiple subsamples of different sizes or proportional contributions of countries.
\end{itemize}

Once that code is done, let's run some R scripts from the command line to quickly summarize the number and/or proportion of sequences by country through time. Alternatively, in RStudio, go to file\textgreater new file\textgreater Rscript, copy/paste code chunk, save and run it there.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#}
\ExtensionTok{R} 

\ExtensionTok{library}\ErrorTok{(}\ExtensionTok{tidyverse}\KeywordTok{)}
\ExtensionTok{library}\ErrorTok{(}\ExtensionTok{RColorBrewer}\KeywordTok{)}

\CommentTok{\#data in}
\ExtensionTok{dat.samp}\OperatorTok{\textless{}}\NormalTok{{-}read.delim}\ErrorTok{(}\StringTok{"data/complete/b117\_seq\_complete\_sampUK.tsv"}\KeywordTok{)}
\ExtensionTok{nrow}\ErrorTok{(}\ExtensionTok{dat.samp}\KeywordTok{)}
\ExtensionTok{dat.samp}\VariableTok{$date}\OperatorTok{\textless{}}\NormalTok{{-}as.Date}\ErrorTok{(}\ExtensionTok{dat.samp}\VariableTok{$date}\KeywordTok{)}

\CommentTok{\#range of dates}
\ExtensionTok{range}\ErrorTok{(}\ExtensionTok{dat.samp}\VariableTok{$date}\KeywordTok{)}

\CommentTok{\#countries }
\ExtensionTok{t.c}\OperatorTok{\textless{}}\NormalTok{{-}table}\ErrorTok{(}\ExtensionTok{dat.samp}\VariableTok{$Country}\KeywordTok{)} \ExtensionTok{\%}\OperatorTok{\textgreater{}}\NormalTok{\% sort}\ErrorTok{(}\ExtensionTok{decreasing}\NormalTok{ = T}\KeywordTok{);}\ExtensionTok{t.c}
\ExtensionTok{order.c}\OperatorTok{\textless{}}\NormalTok{{-}names}\ErrorTok{(}\ExtensionTok{t.c}\KeywordTok{)} \CommentTok{\#order of countries w/ most to least seqs/samples}
\ExtensionTok{n.c}\OperatorTok{\textless{}}\NormalTok{{-}length}\ErrorTok{(}\ExtensionTok{unique}\ErrorTok{(}\ExtensionTok{dat.samp}\VariableTok{$Country}\KeywordTok{));}\ExtensionTok{n.c}

\CommentTok{\#more specific geos also available}
\ExtensionTok{table}\ErrorTok{(}\ExtensionTok{dat.samp}\VariableTok{$Geo\_Location}\KeywordTok{)} \ExtensionTok{\%}\OperatorTok{\textgreater{}}\NormalTok{\% sort}\ErrorTok{(}\ExtensionTok{decreasing}\NormalTok{ = T}\KeywordTok{)}
\ExtensionTok{length}\ErrorTok{(}\ExtensionTok{unique}\ErrorTok{(}\ExtensionTok{dat.samp}\VariableTok{$Geo\_Location}\KeywordTok{))}

\CommentTok{\#setup colors for countries}
\ExtensionTok{country.cols}\OperatorTok{\textless{}}\NormalTok{{-}colorRampPalette}\ErrorTok{(}\ExtensionTok{brewer.pal}\ErrorTok{(}\VariableTok{n}\OperatorTok{=}\NormalTok{8,name=}\StringTok{"Dark2"}\KeywordTok{))(}\ExtensionTok{n.c}\KeywordTok{)}
\ExtensionTok{names}\ErrorTok{(}\ExtensionTok{country.cols}\KeywordTok{)}\OperatorTok{\textless{}}\NormalTok{{-}order.c}

\CommentTok{\#factor and levels specifies order in legend}
\ExtensionTok{dat.samp}\VariableTok{$Country}\OperatorTok{\textless{}}\NormalTok{{-}factor}\ErrorTok{(}\ExtensionTok{dat.samp}\VariableTok{$Country}\ExtensionTok{,}\NormalTok{ levels=order.c}\KeywordTok{)}

\CommentTok{\#plot country contribs}
\ExtensionTok{ggplot}\ErrorTok{(}\ExtensionTok{dat.samp}\KeywordTok{)} \ExtensionTok{+}
  \ExtensionTok{geom\_bar}\ErrorTok{(}\ExtensionTok{aes}\ErrorTok{(}\VariableTok{x}\OperatorTok{=}\NormalTok{Country, }\VariableTok{fill}\OperatorTok{=}\NormalTok{Country}\KeywordTok{))}\ExtensionTok{+}
  \FunctionTok{theme\_bw()}\ExtensionTok{+}
  \ExtensionTok{labs}\ErrorTok{(}\VariableTok{x}\OperatorTok{=}\StringTok{"Country"}\NormalTok{, }\VariableTok{y}\OperatorTok{=}\StringTok{"N sample"}\NormalTok{, }\VariableTok{fill}\OperatorTok{=}\StringTok{"Country"}\KeywordTok{)}\ExtensionTok{+}
  \ExtensionTok{scale\_fill\_manual}\ErrorTok{(}\VariableTok{values}\OperatorTok{=}\NormalTok{country.cols}\KeywordTok{)}\ExtensionTok{+}
  \ExtensionTok{theme}\ErrorTok{(}\ExtensionTok{axis.text.x=element\_text}\ErrorTok{(}\VariableTok{angle}\OperatorTok{=}\NormalTok{45,vjust=1,hjust=1}\KeywordTok{)}\ExtensionTok{,}
        \ExtensionTok{legend.position=}\StringTok{"none"}\KeywordTok{)}
\ExtensionTok{ggsave}\ErrorTok{(}\ExtensionTok{paste0}\ErrorTok{(}\StringTok{"data/complete/samples\_country.png"}\KeywordTok{))}

\CommentTok{\#plot it over time}
\ExtensionTok{ggplot}\ErrorTok{(}\ExtensionTok{dat.samp}\KeywordTok{)} \ExtensionTok{+}
  \ExtensionTok{geom\_bar}\ErrorTok{(}\ExtensionTok{aes}\ErrorTok{(}\VariableTok{x}\OperatorTok{=}\NormalTok{date, }\VariableTok{fill}\OperatorTok{=}\NormalTok{Country}\KeywordTok{))}\ExtensionTok{+}
  \FunctionTok{theme\_bw()}\ExtensionTok{+}
  \ExtensionTok{labs}\ErrorTok{(}\VariableTok{x}\OperatorTok{=}\StringTok{"Sample collection date"}\NormalTok{, }\VariableTok{y}\OperatorTok{=}\StringTok{"N sample"}\NormalTok{, }\VariableTok{fill}\OperatorTok{=}\StringTok{"Country"}\KeywordTok{)}\ExtensionTok{+}
  \ExtensionTok{scale\_fill\_manual}\ErrorTok{(}\VariableTok{values}\OperatorTok{=}\NormalTok{country.cols}\KeywordTok{)}
  
\CommentTok{\#save it}
\ExtensionTok{ggsave}\ErrorTok{(}\ExtensionTok{paste0}\ErrorTok{(}\StringTok{"data/complete/samples\_time\_country.png"}\KeywordTok{))}
\end{Highlighting}
\end{Shaded}

If you are in command line R, enter q() to get out.

\begin{itemize}
\tightlist
\item
  How many sequences were included from the UK?
\item
  What is the range of sample collection dates in the final dataset?
\item
  How many countries were represented in the final dataset?
\end{itemize}

\begin{itemize}
\tightlist
\item
  89
\item
  2020-09-20 to 2021-01-01
\item
  17 countries
\end{itemize}

\subsection{4. Maximum likelihood phylodynamic analysis}\label{maximum-likelihood-phylodynamic-analysis}

The overall goal of this lab is to estimate where and when B.1.1.7 most likely originated. To do this, we will mostly make use of the \href{https://docs.nextstrain.org/projects/augur/en/stable/index.html}{Augur} maximum likelihood tool suite, which powers the \href{https://nextstrain.org/}{NextStrain} website. Within augur, the key tool actually being used for most of our inferences is \href{https://treetime.readthedocs.io/en/latest/}{TreeTime} a likelihood based tool for inferring molecular clocks and ancestral traits.

\textbf{An overview of the basic usage of Augur (figure from the Augur documentation):}
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/augur_analysis_sketch.webp}}

The steps/functions we use are in this section of the lab are the following:

\begin{itemize}
\tightlist
\item
  \textbf{align}: This step constructs a multiple sequence alignment from this subset of genomes.
\item
  \textbf{index}: This step indexes the sequences, counting the frequency of nucleotides and gaps.
\item
  \textbf{filter}: This step filters out sequences that have too many gaps or ambiguous nucleotides.
\item
  \textbf{tree}: This step builds a phylogenetic tree, where branch lengths are measured in substitutions/site (a divergence tree).
\item
  \textbf{refine}: This step constructs a time tree using our existing tree alongside collection dates of SARS-CoV-2 genomic samples (branch lengths are measured in time).
\item
  \textbf{traits}: This step uses treetime's mugration model to infer ancestral traits based on the tree. We will use this to infer ancestral host and geography.
\item
  \textbf{export}: This step exports the data to be used by \href{https://auspice.us/}{Auspice}, a version of the visualization system used by NextStrain that lets you examine your own phylogenetic tree and metadata.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{\texorpdfstring{4.1. Construct a multiple sequence alignment of the genomes (\texttt{augur\ align})}{4.1. Construct a multiple sequence alignment of the genomes (augur align)}}\label{construct-a-multiple-sequence-alignment-of-the-genomes-augur-align}

The first step is to construct a multiple sequence alignment of the genomes, which is required before building a phylogenetic tree. We will be using the command \texttt{augur\ align} to accomplish this task, but underneath this \href{https://mafft.cbrc.jp/alignment/software/}{mafft} runs to construct the alignment.

To build a new folder and construct the alignment, please run the following:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ data/aligned}

\ExtensionTok{augur}\NormalTok{ align }\AttributeTok{{-}{-}nthreads}\NormalTok{ 2 }\AttributeTok{{-}{-}sequences}\NormalTok{ data/complete/b117\_seq\_complete\_sampUK.fasta }\AttributeTok{{-}{-}reference{-}sequence}\NormalTok{ data/download/MN908947.3.fasta }\AttributeTok{{-}{-}output}\NormalTok{ data/aligned/b117\_seq\_complete\_sampUK\_aligned.fasta}
\end{Highlighting}
\end{Shaded}

You should expect to see the following:

\textbf{Output}

\begin{verbatim}
using mafft to align via:
    mafft --reorder --anysymbol --nomemsave --adjustdirection --thread 2 data/b117_seq_complete_sampUK_aligned.fasta.to_align.fasta 1> data/b117_seq_complete_sampUK_aligned.fasta 2> data/b117_seq_complete_sampUK_aligned.fasta.log 

    Katoh et al, Nucleic Acid Research, vol 30, issue 14
    https://doi.org/10.1093%2Fnar%2Fgkf436

WARNING: 1bp insertion at ref position 200 was due to 'N's or '?'s in provided sequences
WARNING: 1bp insertion at ref position 6337 was due to 'N's or '?'s in provided sequences
Trimmed gaps in MN908947.3|China|2019-12-26 from the alignment
\end{verbatim}

The meaning of each parameter:

\begin{itemize}
\tightlist
\item
  `--nthreads: Number of threads for the alignment.
\item
  `--sequences: The input set of sequences in FASTA format.
\item
  `--output: The output alignment, in FASTA format.
\item
  `--reference-sequence: The reference genome (MN908947.3==Wuhan-Hu-1).
\end{itemize}

The reference sequence will be included in our alignment and \texttt{augur\ align} removes any insertions with respect to this reference genome (useful when identifying and naming specific mutations later on in the augur pipeline).

The aligned fasta file is a very similar format as the input file, but the difference is that sequences have been aligned (possibly by inserting gaps \texttt{-}). This also means that all sequences in \texttt{alignment.fasta} should have the same length (whereas sequences in \texttt{sequences.fasta}, which is not aligned, may have different lengths). When time allows, it is good practice to visually inspect the alignment in a viewer such as \href{https://github.com/AliView/AliView}{AliView}.

\subsubsection{\texorpdfstring{4.2. Index sequences (\texttt{augur\ index})}{4.2. Index sequences (augur index)}}\label{index-sequences-augur-index}

This step is optional but useful to inspect sequence quality and quicken the subsequent filtering step, by counting the frequency of nucleotides and gaps.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{augur}\NormalTok{ index }\AttributeTok{{-}{-}sequences}\NormalTok{ data/aligned/b117\_seq\_complete\_sampUK\_aligned.fasta  }\AttributeTok{{-}{-}output}\NormalTok{  data/aligned/b117\_seq\_complete\_sampUK\_aligned\_sequence\_index.tsv}
\end{Highlighting}
\end{Shaded}

Inspect the index output in the `aligned' subfolder.

\begin{itemize}
\tightlist
\item
  Knowing that the genome size is 29,903 bp, do any samples appear to have more gaps or Ns than you would expect?
\end{itemize}

\begin{itemize}
\tightlist
\item
  At first glance, none appear to have \textgreater\textasciitilde3000 gaps or Ns.
\end{itemize}

\subsubsection{\texorpdfstring{4.3. Filter out low quality sequences (\texttt{augur\ filter})}{4.3. Filter out low quality sequences (augur filter)}}\label{filter-out-low-quality-sequences-augur-filter}

Filter seqs to a minimum length (--min-length) of 26913, which represents 90\% of 29,903 sites with ACTG (i.e., max 10\% ambig and gaps)

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ data/filtered}

\ExtensionTok{augur}\NormalTok{ filter }\AttributeTok{{-}{-}nthreads}\NormalTok{ 2 }\AttributeTok{{-}{-}sequences}\NormalTok{ data/aligned/b117\_seq\_complete\_sampUK\_aligned.fasta }\AttributeTok{{-}{-}metadata}\NormalTok{ data/complete/b117\_seq\_complete\_sampUK.tsv }\AttributeTok{{-}{-}sequence{-}index}\NormalTok{ data/aligned/b117\_seq\_complete\_sampUK\_aligned\_sequence\_index.tsv }\AttributeTok{{-}{-}min{-}length}\NormalTok{ 26913 }\AttributeTok{{-}{-}output{-}sequences}\NormalTok{ data/filtered/b117\_seq\_complete\_sampUK\_aligned\_filt.fasta }\AttributeTok{{-}{-}output{-}metadata}\NormalTok{ data/filtered/b117\_seq\_complete\_sampUK\_filt.tsv }
\end{Highlighting}
\end{Shaded}

\textbf{Output}

\begin{verbatim}
1 strain was dropped during filtering
    1 had no metadata (this is the refence sequence)
178 strains passed all filters
\end{verbatim}

Nice, all the sequences passed our quality filters (this is rarer than you might think).

\begin{itemize}
\tightlist
\item
  Why is it important to remove low quality sequences prior to phylogenetic inference?
\item
  The choice of maximum 10\% ambiguous and gaps seems (\emph{is}) somewhat arbitrary. How would increasing or decreasing this threshold change the analysis?
\end{itemize}

\begin{itemize}
\tightlist
\item
  Sequences with excessive gaps or ambiguous sites may not have as high of accuracy, and have less signal to contribute to the tree, which could bias its placement.
\item
  Increasing the threshold would allow more sequences of lower quality to pass through the filter. Decreasing the threshold is more conservative.
\end{itemize}

\subsubsection{4.4. Mask problematic sites in the alignment}\label{mask-problematic-sites-in-the-alignment}

Not all sites in the genome are equally informative towards inferring their phylogenetic relationships. Some sites can be misleading and should be masked before building trees. The ends of alignments often exhibit low coverage and a higher rate of sequencing or mapping errors. In SARS-CoV-2, this includes positions 1--55 and 29804--29903. Homoplasic sites at which multiple recurrent mutations arise may reflect contamination, recurrent sequencing errors, or hypermutability. Even if they are accurate, they may not be representative of evolution across the remainder of the genome. Another example (not relevant here) would be sites at which drug resistance mutations occur. In HIV-1 phylodynamic analyses, codons of surveillance drug resistance mutations are masked. For further interest, see this \href{https://virological.org/t/masking-strategies-for-sars-cov-2-alignments/480}{justification for masking SARS-CoV-2 sequences}.

Note that augur also has a masking subcommand, but it requires the mask file to be in a different format, so we're not using it here. Instead, we're going to run a python script that masks sequences based on a vcf (Variant Call Format) file of problematic sites. In order to run this, we have to concatenate the reference sequence back into the fasta, which was filtered out by augur above. Then we're going to run our masking script, and then remove that reference again.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ data/masked}

\CommentTok{\#\# Concatenate the fasta with the reference fasta}
\FunctionTok{cat}\NormalTok{ data/filtered/b117\_seq\_complete\_sampUK\_aligned\_filt.fasta data/download/MN908947.3.fasta }\OperatorTok{\textgreater{}}\NormalTok{ data/filtered/b117\_seq\_complete\_sampUK\_aligned\_filt\_ref.fasta}

\CommentTok{\#\# Run the masking python script}
\ExtensionTok{python3}\NormalTok{ scripts/mask\_alignment\_using\_vcf.py }\AttributeTok{{-}i}\NormalTok{ data/filtered/b117\_seq\_complete\_sampUK\_aligned\_filt\_ref.fasta }\AttributeTok{{-}o}\NormalTok{ data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_ref\_mask.fasta }\AttributeTok{{-}v}\NormalTok{ scripts/problematic\_sites\_vcfv5.vcf }\AttributeTok{{-}n} \StringTok{"n"} \AttributeTok{{-}{-}reference\_id} \StringTok{"MN908947.3|China|2019{-}12{-}26"}

\CommentTok{\#\# Remove the reference sequence from the alignment using augur filter}
\ExtensionTok{augur}\NormalTok{ filter }\AttributeTok{{-}{-}sequences}\NormalTok{ data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_ref\_mask.fasta }\AttributeTok{{-}{-}metadata}\NormalTok{ data/filtered/b117\_seq\_complete\_sampUK\_filt.tsv }\AttributeTok{{-}{-}output{-}sequences}\NormalTok{ data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.fasta }\AttributeTok{{-}{-}output{-}metadata}\NormalTok{ data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.tsv}
\end{Highlighting}
\end{Shaded}

The \$cat command is a simple and useful tool to concatenate multiple fasta files together.

Let's have a closer look at the parameters we used in the python script:

\begin{itemize}
\tightlist
\item
  \texttt{-i}: Fasta input alignment to be masked (with the ref sequence in it)
\item
  \texttt{-o}: Fasta output alignment with problematic sites masked
\item
  \texttt{-v}: The vcf file of problematic sites from \href{https://virological.org/t/masking-strategies-for-sars-cov-2-alignments/480}{de Maio}
\item
  \texttt{-n}: the character to replace nucleotides at the masked sites
\item
  \texttt{-\/-reference-id}: the ID of the reference sequence in the input
\end{itemize}

\subsubsection{Optional}\label{optional}

\begin{itemize}
\tightlist
\item
  Open and inspect the fasta files before and after masking.
\end{itemize}

\subsubsection{\texorpdfstring{4.5. Build a maximum liklihood phylogenetic tree (\texttt{augur\ tree})}{4.5. Build a maximum liklihood phylogenetic tree (augur tree)}}\label{build-a-maximum-liklihood-phylogenetic-tree-augur-tree}

The next step is to take the set of aligned genomes and build a phylogenetic tree (a divergence tree in units of substitutions per site). We will use \texttt{augur\ tree} for this, but underneath it (by default) runs \href{http://www.iqtree.org/}{iqtree}, which uses maximum likelihood to infer a phylogenetic tree.

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ trees}

\ExtensionTok{augur}\NormalTok{ tree }\AttributeTok{{-}{-}nthreads}\NormalTok{ 4 }\AttributeTok{{-}{-}method}\NormalTok{ iqtree }\AttributeTok{{-}{-}substitution{-}model}\NormalTok{ GTR }\AttributeTok{{-}{-}alignment}\NormalTok{ data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.fasta }\AttributeTok{{-}{-}output}\NormalTok{ trees/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.tre}
\end{Highlighting}
\end{Shaded}

Let's check out the parameters

\begin{itemize}
\tightlist
\item
  \texttt{-\/-nthreads}: Number of threads
\item
  \texttt{-\/-method}: The tree inference method. We use the default, iqtree(3)
\item
  \texttt{-\/-substitution-model}: the underlying substitution model, here specified as generalized time reversible (GTR), which means that each nucleotide substitution rate in the matrix (ACTG vs ACTG) is separately estimated.
\item
  \texttt{-\/-alignment}: the input alignment (with masked sites)
\item
  \texttt{-\/-output}: the output tree (can be .tre or .nwk files)
\end{itemize}

You should expect to see this (or a very similar) output:

\textbf{Output}

\begin{verbatim}
Building a tree via:
    iqtree3 --threads-max 4 -s data/masked/b117_seq_complete_sampUK_aligned_filt_mask-delim.fasta -m GTR --ninit 2 -n 2 --epsilon 0.05 -T AUTO --redo > data/masked/b117_seq_complete_sampUK_aligned_filt_mask-delim.iqtree.log
    Please use the corresponding citations according to
    https://iqtree.github.io/doc/Home#how-to-cite-iq-tree

Building original tree took 12.720907926559448 seconds
\end{verbatim}

This produces as output a \texttt{.tre} file, which is a Newick format phylogenetic tree, which has been midpoint-rooted. You can load this file in a variety of phylogenetic tree viewers (such as \url{http://phylo.io/}) but we will further refine this file to work with Auspice.

Another output file is \texttt{alignment-delim.iqtree.log}, which contains additional information from \href{http://www.iqtree.org/}{iqtree}. You can take a look at this file to get an idea of what \href{http://www.iqtree.org/}{iqtree} was doing by using \texttt{tail} (prints the last few lines of a file).

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail} \AttributeTok{{-}n}\NormalTok{ 20 data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask{-}delim.iqtree.log}
\end{Highlighting}
\end{Shaded}

\textbf{Output}

\begin{verbatim}
Optimal log-likelihood: -44067.898
Rate parameters:  A-C: 0.24165  A-G: 1.07984  A-T: 0.21863  C-G: 0.31202  C-T: 3.23196  G-T: 1.00000
Base frequencies:  A: 0.299  C: 0.184  G: 0.196  T: 0.321
Parameters optimization took 1 rounds (0.019 sec)
BEST SCORE FOUND : -44067.898
Total tree length: 0.012

Total number of iterations: 2
CPU time used for tree search: 1.277 sec (0h:0m:1s)
Wall-clock time used for tree search: 0.408 sec (0h:0m:0s)
Total CPU time used: 29.925 sec (0h:0m:29s)
Total wall-clock time used: 12.023 sec (0h:0m:12s)

Analysis results written to: 
  IQ-TREE report:                data/masked/b117_seq_complete_sampUK_aligned_filt_mask-delim.fasta.iqtree
  Maximum-likelihood tree:       data/masked/b117_seq_complete_sampUK_aligned_filt_mask-delim.fasta.treefile
  Likelihood distances:          data/masked/b117_seq_complete_sampUK_aligned_filt_mask-delim.fasta.mldist
  Screen log file:               data/masked/b117_seq_complete_sampUK_aligned_filt_mask-delim.fasta.log

Date and Time: Wed Nov 19 15:50:14 2025
\end{verbatim}

As iqtree uses a maximum likelihood approach, you will see that it will report the likelihood score of the optimal tree (reported as log-likelihoods since likelihood values are very small).

\emph{Note: For this lab we are not looking at branch support values for a tree, but for real-world analysis you may wish to look into including (ultra-fast) bootstrap support values or approximate likelihood ratio test values. This will give a measure of how well-supported each branch in the tree is by the alignment (often a number from 0 for little support to 100 for maximal support). Please see the \href{http://www.iqtree.org/doc/Tutorial\#assessing-branch-supports-with-ultrafast-bootstrap-approximation}{IQTree documentation} for more details.}

\subsubsection{\texorpdfstring{4.6. Inferring timing of host change (\texttt{augur\ refine})}{4.6. Inferring timing of host change (augur refine)}}\label{inferring-timing-of-host-change-augur-refine}

The tree output by \href{http://www.iqtree.org/}{iqtree} shows hypothetical evolutionary relationships between different SARS-CoV-2 genomes with branch lengths representing distances between different genomes (in units of \textbf{substitutions/site}, i.e., the predicted number of substitutions between genomes divided by the alignment length). However, other methods of measuring distance between genomes are possible. In particular we can incorporate the collection dates of the different SARS-CoV-2 genomes to infer a tree where branches are scaled according to the elapsed time and the dates of internal nodes are inferred. Such trees are called \textbf{time trees} or \textbf{time-scaled phylogenies}.

We will use \href{https://treetime.readthedocs.io/en/latest/}{TreeTime} invoked by \href{https://docs.nextstrain.org/projects/augur/en/stable/index.html}{augur} to infer a \textbf{time tree} from our phylogenetic tree using collection dates of the SARS-CoV-2 genomes stored in the metadata file. We will use the \texttt{augur\ refine} step to run TreeTime and perform some additional refinemint of the tree.

To do this, please run the following:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{augur}\NormalTok{ refine }\AttributeTok{{-}{-}alignment}\NormalTok{ data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.fasta }\AttributeTok{{-}{-}tree}\NormalTok{ trees/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.tre }\AttributeTok{{-}{-}metadata}\NormalTok{ data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.tsv }\AttributeTok{{-}{-}timetree} \AttributeTok{{-}{-}divergence{-}units}\NormalTok{ mutations }\AttributeTok{{-}{-}output{-}tree}\NormalTok{ trees/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask\_timetree.tre }\AttributeTok{{-}{-}output{-}node{-}data}\NormalTok{ trees/refine.node.json }\AttributeTok{{-}{-}seed}\NormalTok{ 123 }\AttributeTok{{-}{-}stochastic{-}resolve} \AttributeTok{{-}{-}root}\NormalTok{ least{-}squares }\AttributeTok{{-}{-}date{-}confidence} \AttributeTok{{-}{-}clock{-}rate}\NormalTok{ 4.5e{-}4 }\AttributeTok{{-}{-}clock{-}std{-}dev}\NormalTok{ 0.2}
\end{Highlighting}
\end{Shaded}

Once again, let's have a closer look at the parameters:

\begin{itemize}
\tightlist
\item
  \texttt{-\/-alignment}: The alignment used to build the tree. Used to re-scale the divergence units.
\item
  \texttt{-\/-tree}: The starting input tree built using iqtree3.
\item
  \texttt{-\/-metadata}: The metadata which contains the SARS-CoV-2 genome names (in a column called \texttt{name}) and the sample collection dates (in a column named \texttt{date}).
\item
  \texttt{-\/-timetree}: Specifiy to build a time tree.
\item
  \texttt{-\/-divergence-units}: Convert the branch lengths of \textbf{substitutions/site} (\textbf{mutations/site}) to \textbf{mutations} (not needed to build a time tree, this is just used for visualizing the tree later on).
\item
  \texttt{-\/-output-tree}: The output Newick file containing the time tree.
\item
  \texttt{-\/-output-node-data}: Augur will store additional information here which will let us convert between time trees and substitution trees.
\item
  \texttt{-\/-seed}: Set a seed for reproducibility
\item
  \texttt{-\/-stochastic-resolve}: Resolve polytomies stochastically (forces a bifurcating tree)
\item
  \texttt{-\/-root}: choose root that optimizes the least squares for the root-to-tip regression
  -date-confidence
\item
  \texttt{-\/-clock-rate}: We are specifying a relaxed molecular clock rate, with a mean of 4.5e-4 based on \href{https://academic.oup.com/ve/article/8/2/veac080/6677185}{Hill et al.~2022}
\item
  \texttt{-\/-clock-std-dev}: This is the deviance on the relaxed clock, allowing for variable evolutionary rates across the tree. You could play with this parameter if you have time.
\end{itemize}

You should expect to see the following as output:

\textbf{Output}

\begin{verbatim}
augur refine is using TreeTime version 0.11.4

4.28    TreeTime.reroot: with method or node: least-squares

4.28    TreeTime.reroot: rerooting will account for covariance and shared ancestry.

5.32    ###TreeTime.run: INITIAL ROUND

9.24    TreeTime.reroot: with method or node: least-squares

9.24    TreeTime.reroot: rerooting will account for covariance and shared ancestry.

9.41    ###TreeTime.run: rerunning timetree after rerooting

13.59   ###TreeTime.run: ITERATION 1 out of 2 iterations

21.88   ###TreeTime.run: ITERATION 2 out of 2 iterations

55.09   ###TreeTime.run: FINAL ROUND - confidence estimation via marginal
        reconstruction
Inferred a time resolved phylogeny using TreeTime:
    Sagulenko et al. TreeTime: Maximum-likelihood phylodynamic analysis
    Virus Evolution, vol 4, https://academic.oup.com/ve/article/4/1/vex042/4794731

updated tree written to trees/b117_seq_complete_sampUK_aligned_filt_mask_timetree.tre
node attributes written to refine.node.json
\end{verbatim}

As output, the file \texttt{output-tree\ trees/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask\_timetree.tre} will contain the time tree while the file \texttt{trees/refine.node.json} contains additional information about the tree. If you want, download the time tree and view it in \href{https://icytree.org/}{icytree}.

\paragraph{Optional}\label{optional-1}

If you have spare time at the end of the lab, consider returning to conduct a sensitivity analysis on the specified clock rate and variance.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{\texorpdfstring{4.7. Infer ancestral states for host and location (\texttt{augur\ traits})}{4.7. Infer ancestral states for host and location (augur traits)}}\label{infer-ancestral-states-for-host-and-location-augur-traits}

We will now be using treetime's \texttt{mugration} model to reconstruct discrete ancestral states. In other words, we are trying to use the tree and genome metadata to reconstruct the most likely country at each of the internal nodes in the tree. It is called a mugration model because it is fundamentally estimating migration between ``demes'' (in this case, countries) as if it were a substitution rate matrix, commonly referred to as \emph{mu}. See \href{https://pubmed.ncbi.nlm.nih.gov/19779555/}{Lemey et al.~2009} if you're interested in learning more.

Side bar: discrete phylogeography, ancestral state reconstruction, ancestral trait inference, and ancestral character estimation can all be used interchangeably in this context.

As with the other analyses, augur makes this process very convenient:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{augur}\NormalTok{ traits }\AttributeTok{{-}{-}tree}\NormalTok{ trees/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask\_timetree.tre }\AttributeTok{{-}{-}metadata}\NormalTok{ data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.tsv }\AttributeTok{{-}{-}columns}\NormalTok{ Country Geo\_Location }\AttributeTok{{-}{-}output{-}node{-}data}\NormalTok{ trees/trait.node.json}
\end{Highlighting}
\end{Shaded}

The parameters we used are:

\begin{itemize}
\tightlist
\item
  \texttt{-\/-tree}: The time-calibrated tree we inferred in step 4
\item
  \texttt{-\/-metadata}: The metadata file with information about the genomes in the tree
\item
  \texttt{-\/-columns}: The columns in the metadata file we want to infer ancestral states for, in this case country and geo\_location (more specific info).
\item
  \texttt{-\/-output-node-data}: Augur stores the additional information related to the ancestral trait inference for later visualisation.
\end{itemize}

You should expect to see the following as output:

\textbf{Output}

\begin{verbatim}
augur traits is using TreeTime version 0.11.4
Assigned discrete traits to 178 out of 178 taxa.

Assigned discrete traits to 178 out of 178 taxa.

Inferred ancestral states of discrete character using TreeTime:
    Sagulenko et al. TreeTime: Maximum-likelihood phylodynamic analysis
    Virus Evolution, vol 4, https://academic.oup.com/ve/article/4/1/vex042/4794731

results written to trait.node.json
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{\texorpdfstring{4.8. Package up data for Auspice visualisation (\texttt{augur\ export})}{4.8. Package up data for Auspice visualisation (augur export)}}\label{package-up-data-for-auspice-visualisation-augur-export}

We will be using \href{https://auspice.us/}{Auspice} to visualize the tree alongside our metadata. To do this, we need to package up all of the data we have so far into a special file which can be used by Auspice. To do this, please run the following command:

\textbf{Commands}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{augur}\NormalTok{ export v2 }\AttributeTok{{-}{-}tree}\NormalTok{ trees/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask\_timetree.tre }\AttributeTok{{-}{-}node{-}data}\NormalTok{ trees/refine.node.json trees/trait.node.json }\AttributeTok{{-}{-}maintainers} \StringTok{"CBW{-}IDE{-}2025"} \AttributeTok{{-}{-}title} \StringTok{"Module 5 Practical"} \AttributeTok{{-}{-}output}\NormalTok{ trees/analysis{-}package.json }\AttributeTok{{-}{-}geo{-}resolutions}\NormalTok{ Country}
\end{Highlighting}
\end{Shaded}

The parameters:

\begin{itemize}
\tightlist
\item
  \texttt{-\/-tree}: The time-calibrated tree
\item
  \texttt{-\/-node-data\ refine.node.json\ trait.node.json}: The 2 jsons created during the temporal and ancestral trait inference
\item
  \texttt{-\/-title\ "Module\ 5\ Practical"}: A title for the auspice page, you can make this anything you want.
\item
  \texttt{-\/-maintainers\ "CBW-IDE-2025"}: A name for who is responsible for this analysis, useful for later, can make this anything you want.
\item
  \texttt{-\/-geo-resolutions\ Country}: Spatial resolution on which to plot the data. In this case we want country
\item
  \texttt{-\/-output\ analysis-package.json}: The key output file we want to generate. The file \texttt{trees/analysis-package.json} contains the tree with different branch length units (time and substitutions), our inferred ancestral traits, as well as additional data.
\end{itemize}

You should expect to see the following as output:

\textbf{Output}

\begin{verbatim}
Trait 'Geo_Location' was guessed as being type 'categorical'. Use a 'config' file if you'd like to set this yourself.
Trait 'Country' was guessed as being type 'categorical'. Use a 'config' file if you'd like to set this yourself.
Validating produced JSON
Validating schema of 'trees/analysis-package.json'...
Validating that the JSON is internally consistent...
Validation of 'trees/analysis-package.json' succeeded.
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{5. Visualizing the phylodynamic analysis}\label{visualizing-the-phylodynamic-analysis}

Now that we've constructed and packaged up a tree (\texttt{analysis-package.json}), we can visualize this data alongside the data augur has extracted from our metadata using augur and \href{https://auspice.us/}{Auspice}.

Note: I've had issues recently getting this to work in firefox so recommend using a chromium-based browser if you have issues.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{5.1. Load data into Auspice}\label{load-data-into-auspice}

To do this, please navigate to \url{http://IP-ADDRESS/module5/} and download the files \texttt{trees/analysis-package.json} and \texttt{data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.tsv} to your computer (if the link does not download you can \textbf{Right-click} and select \textbf{Save link as\ldots{}}).

Next, navigate to \url{https://auspice.us/} and drag the file \texttt{analysis-package.json} onto the page.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{5.2. Explore data}\label{explore-data}

Spend some time to explore the data and get used to the Auspice interface. Try switching between different \textbf{Tree Layouts}, \textbf{Branch Lengths}, or colouring the tree by different criteria in the metadata table. This is always worth doing a bit before diving into analyses. Scroll down and hit download to save the outputs of this for future reference. The nexus trees contain the internal node states along with the tree, so is readable into R and FigTree if you want to customize the tree more or analyze ancestral state transitions to identify sublineages or calculate dispersion rates etc.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{5.3. Examine the molecular clock}\label{examine-the-molecular-clock}

Now we are going to look more at the Branch length ``TIME'' and ``DIVERGENCE'' option.

First, let's look at the divergence tree and use the x-axis to work out how many mutations there are on certain branches. We can use the x-axis in the ``RECTANGULAR'' layout of the ``DIVERGENCE'' tree to work this out or we can directly move our cursor over the node/branch of interest to have a pop-up with the details.

\textbf{The mutation tree:}
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/auspice_muts.png}}

Now, we can do the same thing if we switch to ``TIME'' and look at the approximate inferred dates of the internal nodes of the tree. We can also mouse over to get the exact pop-up dates.

\begin{itemize}
\tightlist
\item
  What was the maximum number of mutations from the root to any tip?
\item
  What was the most likely date of the root (AKA the \textbf{time of the most recent common ancestor (TMRCA)}) ? Confidence interval?
\item
  How do you interpret the root of this phylogeny? Is it representative of the entire B.1.1.7 lineage?
\end{itemize}

\begin{itemize}
\tightlist
\item
  18 (to tip MW598424.1/Ghana/2020-12-22). This tip could be considered a temporal outlier and excluded from the analysis.
\item
  2020-08-21, but the confidence interval is wide and lower bound is unrealistic (2016-05-31 - 2020-09-12)
\item
  This root is the most recent common ancestor (MRCA) of a random sample of early B.1.1.7 sequences, downsampling the UK samples to an equal number as non-UK samples. It may not be fully representative of the full diversity of early B.1.1.7, nor later evolution of B.1.1.7.
\end{itemize}

Now, have a look at the ``CLOCK'' layout/figure found in the options bar on the left of your screen. This brings up a \textbf{root-to-tip regression} from the divergence tree (i.e., how many mutations from root to tip, versus collection date). You can click on individual points (including the upper outlier from Ghana) to see the average substitution rate for that tip. for that tip.

\textbf{The root-to-tip regression plot:}
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/auspice_rtt.png}}

\begin{itemize}
\tightlist
\item
  What is the slope of the root-to-tip regression (in subs/year; subs/site/year)? What does it represent?
\item
  Does the trend line for the root-to-tip regression look like a good fit with good temporal signal?
\item
  Given the data and trendline, how accurate do you think the inferred root date is?
\item
  How might you estimate the uncertainty in these estimates?
\end{itemize}

\begin{itemize}
\tightlist
\item
  14.9 substitutions per year. Or (ignoring masked sites), 14.9/29903=0.0004982778=5.0 subs/site/year. It represents the strict molecular clock rate.
\item
  Aside from the outlier, it looks like a good fit (an R-squared value should really be reported) with reasonable temporal signal.
\item
  The confidence interval on the root is wide, we have a relatively small (singular) sample, but the linear fit is decent and we included early samples with higher probabilities, so I'd say medium to low accuracy for the mean estimate.
\item
  Multiple subsamples of higher sample sizes. Bootstrapping/ultrafast bootstrapping. Multiple clock rate assumptions when inferring the time tree.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection{5.4. Examine the ancestral state reconstruction}\label{examine-the-ancestral-state-reconstruction}

We can colour the tree and rename the tree tip labels using the metadata to reflect the ancestral state reconstruction. The internal nodes/branches will be coloured based on the inferred ancestral state, in this case Country or Geo\_location. Investigate the ancestral states at internal nodes throughout the tree including at the root. Use the play button to visualize the dispersion globally.

\textbf{Here is a look at what you should be seeing in auspice when you change branches to be colored by Country and choose the multi-panel layout.}
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/auspice_geo.png}}

\begin{itemize}
\tightlist
\item
  Based on this analysis, where was the root of B.1.1.7 likely to have occurred?
\item
  How do you think sequence subsampling might have impacted this analysis?
\item
  What additional data would give you more confidence in asserting where this lineage was likely to have originated?
\item
  How could you use the output of this analysis to quantify viral dispersion?
\end{itemize}

\begin{itemize}
\tightlist
\item
  The United Kingdom
\item
  We included far fewer sequences from the UK than were available, and still inferred it as the likely origin. It would be worth comparing other sampling schemes to validate this, but we have high confidence that this was the likely origin, given the sequences publicly available.
\item
  More early B.1.1.7 sequences from other countries would be informative. Also, including close parental or sister lineages (like B.1 similar to B.1.1.7) of similar time/place could add resolution to the analysis.
\item
  Quantify importation or exportation rates by country through time. You could also incorporate population size, connectivity (mobility data), geographical distance, and other factors to evaluate (or control for) factors associated with elevated dispersal.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

That's that for our maximum likelihood phylogeographic inference. Now, we're going to compare those results to a Bayesian approach.

\subsection{6. Delphy analysis}\label{delphy-analysis}

\href{https://delphy.fathom.info/}{Delphy} is a newly released Bayesian phylogenetics tool designed to be fast, scalable, accessible, and accurate. Delphy is faster than other Bayesian tools because it uses Explicit Mutation-Annotated Trees (EMAT), which are hypotheses of how an ancestral sequence replicated and evolved, mutation by mutation, into the observed dated sequences. Check out their \href{https://www.biorxiv.org/content/10.1101/2025.03.25.645253v1.full.pdf}{pre-print} if you want more details.

In Bayesian phylogenetic (and phylodynamic) inference, many models (made up of trees and parameters) are sampled to generate posterior probability distributions, and our estimates will reflect the \textbf{highest posterior density (HPD)}. Bayesian analyses differ from max likelihood approaches in that they incorporate uncertainty from the tree inference and sampling process, and rely on specification of prior probabilities for the parameters to be explored using \textbf{Markov Chain Monte Carlo (MCMC)}, which is basically a hill-climbing algorithm that suggests and compares models compared to the previous step. Model convergence is assessed using minimum \href{https://github.com/fathominfo/delphy-web/wiki/Effective-Sample-Size}{\textbf{effective sampling size (ESS)}} reflects and in robust analyses is usually \textasciitilde200 after removing the burn-in. Burn-in are steps that you don't include in the final posterior distributions because they represent the model stabilization period. In a robust Bayesian analyses, multiple \textbf{MCMC chains} are run, and then combined after burn-in.

The tree displayed reflects a \textbf{maximum clade credibility} tree, which is a single tree with the highest posterior probability across the Bayesian posterior distribution of trees. Branch width represents the credibility threshold, more commonly referred to as \textbf{posterior support or probability}, which indicate the strength of evidence for clades, given the data and model used.

\subsubsection{6.1. Modify fasta names, then load the renamed fasta into Delphy}\label{modify-fasta-names-then-load-the-renamed-fasta-into-delphy}

Delphy wants sequence names to include info separated by ``\textbar{}'' not ``/'', so we'll start by running a simple R script to rename the relevant fasta and metadata.

Run this Rscript in the command line. You can go to scripts/convert\_names.R to have a look at what this script does, but it's pretty simple.

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{Rscript}\NormalTok{ scripts/convert\_names.R }\StringTok{"data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.tsv"} \StringTok{"data/masked/b117\_seq\_complete\_sampUK\_aligned\_filt\_mask.fasta"}
\end{Highlighting}
\end{Shaded}

The renamed files output goes to a newly created folder, \texttt{data/delphy}.

Now, open the \href{delphy.fathom.info}{Delphy web browser} and drag/drop or upload the fasta file \texttt{data/delphyb117\_seq\_complete\_sampUK\_aligned\_filt\_mask\_delphy.fasta}.

\subsubsection{6.2. Modify priors by clicking on advanced options}\label{modify-priors-by-clicking-on-advanced-options}

Leave the sampling frequency to sample 1 tree per 1,000,000 steps. You could change this to sample more frequently if you want, but this might not improve the speed of convergence if trees are too similar to each other.

Let's modify the priors. It is important to assess and modify priors in Bayesian analyses to reflect your \emph{prior} expectations and narrow the parameter space explored. Improper priors not only slow down convergence, but can bias analyses.

\begin{itemize}
\tightlist
\item
  For mutation rate:

  \begin{itemize}
  \tightlist
  \item
    enable site rate heterogeneity
  \item
    fix overall (mean) mutation rate
  \item
    set initial mutation rate to 45e-5 s/s/y 4.5e-4 (Hill et al.~2022)
  \end{itemize}
\item
  Population model

  \begin{itemize}
  \tightlist
  \item
    Choose skygrid (these are both coalescent tree priors. Delphy does not have birth-death models like BEAST does.)
  \item
    model start date 2020-01-01 (the earliest this lineage root could have occurred)
  \item
    split into 10 intervals
  \item
    keep other parameters the same (could play with advanced skygrid parameters)

    \begin{itemize}
    \tightlist
    \item
      \href{https://github.com/broadinstitute/delphy/wiki/Flexible-population-models}{Further reading on the choice of population model}
    \end{itemize}
  \end{itemize}
\end{itemize}

\textbf{Here is a look at the advanced options you should be modifying. This should also have had split into 10 intervals.}
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/delphy_prior.png}}

Before we start the run, go to Customize in the toolbar, then add the metadata file \texttt{data/delphyb117\_seq\_complete\_sampUK\_aligned\_filt\_mask\_delphy.tsv} by drag/drop then in Color system, click metadata, color by Country, then select all.

\textbf{Delphy customize options:}
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/delphy_cust.png}}

\begin{itemize}
\tightlist
\item
  what biases may we have imposed through our selection of priors?
\end{itemize}

\begin{itemize}
\tightlist
\item
  We may bias our estimates of the root date by specifying an improper mean molecular clock rate. The skygrid may not fit well with 10 categories (not enough data in early period). The underlying population may be better described by the exponential population distribution. Any misspecified or over-generalized prior has the potential to bias the analysis or underestimate the true uncertainty in the estimates.
\end{itemize}

\subsubsection{6.3. Run the analysis and interpret output}\label{run-the-analysis-and-interpret-output}

Hit play and watch in \emph{statistical awe} as trees sampled from the posterior are displayed in real-time, alongside model parameters including the number of mutations, ln(posterior), total evolutionary time (tree height), and effective population size in years. We can start to investigate the output before the run is complete (or you can upload the precomputed-analysis/trees/delphy\_out/.delphy file directly to the Delphy home page to analyze a more complete analysis of \textgreater300 million steps).

\textbf{Here is a look at the more complete run, including a sampled tree, the MCC tree, and the posterior distribution of some key model parameters.}
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/delphy_mcc.png}}

Although Delphy does not seem very good at estimating the ESS and assessing convergence, a good sign of convergence is when the posterior distributions of parameters appear as ``fuzzy caterpillars'' (converging around similar values, as opposed to jumping around).

\begin{itemize}
\item
  How do you interpret the change in effective population size through time?
\item
  Did your model converge? How can you tell?
\end{itemize}

\begin{itemize}
\item
  The effective population size is low and then increases rapidly (exponentially?) as this lineage transmitted broadly in the UK and globally, but then appears to decrease at the end of the study perio, but this likely reflects lower sampling rate towards the end of the period.
\item
  After removing burn-in, the ESS for the parameters shown are below 200, so probably not. We would have to run this for more steps, and maybe for multiple chains that we could combine using a tool like \href{https://www.beast2.org/tracer-2/}{Tracer}.
\end{itemize}

NEXT, let's click on Lineages in the upper tool bar. In the bottom left corner, you can change branch coloring between Credibility and Metadata (Country). You can also change the credibility threshold (posterior support) for nodes'/branches' shading in the tree displayed.

\begin{itemize}
\item
  Is the first bifurcation at the root well-supported?
\item
  What is the median posterior root date?
\end{itemize}

\begin{itemize}
\item
  No, the credibility threshold is \textless50\%.
\item
  Median posterior root date \textasciitilde19 July 2020. Normally a 95\% HPD should be reported, but Delphy does not show that directly. If you have time/interest, you can read the exported data into R and estimate the uncertainty.
\end{itemize}

And now click on Mutations in the upper tool bar. Click on recurrent mutations, then A17615G. This mutation may have evolved independently multiple times, though it was only found in 70 genomes. It is a non-synonymous mutation in the ORF1b gene, resulting in an amino acid change of Lysine to Arginine (K1383R) in the nsp10 protein. Explore the other mutations that occurred in the phylogeny.

\begin{itemize}
\item
  Where does the Delphy analysis suggest the root of this B.1.1.7 phylogeny occurred?
\item
  Does the Bayesian analysis corroborate the maximum likelihood analysis in terms of the time and place of origin of B.1.1.7?
\end{itemize}

\begin{itemize}
\item
  The UK
\item
  Yes! Both the Bayesian and ML analyses suggest this lineage originated around August 20, 2020 in the UK. This is very close to the estimate from \href{https://academic.oup.com/ve/article/8/2/veac080/6677185}{Hill et al.~2022} of a ``TMRCA of the Alpha clade estimated at 28 August 2020 (95\% HPD: 15th August 2020 to 9th September 2020)''.
\end{itemize}

\subsubsection{6.4. Export results}\label{export-results}

To wrap this Delphy analysis up, go to Customize, then in the bottom left, export the Tree as png and Newick file, and the Delphy run. The Delphy run file can be dropped back into Delphy or read into R. You can also generate input/output to BEAST to run a more \emph{bespoke} Bayesian analysis.

\subsection{7. End of lab}\label{end-of-lab}

You've made it to the end of the lab. Awesome job.

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{img/PGE_mod5/end.png}}
\caption{treat}
\end{figure}

If you have some extra time, you can explore Auspice or Delphy further, or see how the results differ if you change the subsampling scheme or the clock rate. Another \emph{fun} activity would be to read the Delphy output into R and summarize the median and 95\% highest posterior distribution of the root date,

\bibliography{book.bib,packages.bib}

\end{document}
